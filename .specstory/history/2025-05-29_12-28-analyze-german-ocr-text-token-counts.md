<!-- Generated by SpecStory -->

# Analyze German OCR text token counts (2025-05-29 14:28:10)

_**User**_

I have a .md file with text that was parsed via ocr. The text is in German, so some words are put together with out the appropriate white spaces. Answer this: From my research, for German (especially with its longer compound words), there is no accurate word splitter. So the best way to process them is via a small llm?
If yes, then answer this please: I have the parsed text in a .md file. I would like to calculate for each line, how many tokens it has, to check if my locally hosted small llm can handle each text, token wise (because I had to set a max model len, I use vllm). I have an open ai compatible api for that, and a method defined in backend/utils/local_call.py. Please write me a script to backend/processing that leverages this tokenise endpoint for each file and prints out how many lines there are in the .md file with more than 1024 tokens, and also prints out the number of the top 5 lines. Here is the function. Let me specify the file path to the .md file at the start of the script.

---

_**Assistant**_

You're absolutely right about German compound words! German does have particularly long compound words (like "DonaudampfschifffahrtsgesellschaftskapitÃ¤n"), and there's no reliable automatic word splitter that can accurately break them down while preserving meaning. Using a small LLM is indeed the best approach for processing such text, as it can understand context and handle compound words appropriately.

Now, let me create a script for you that uses your tokenization endpoint to analyze the `.md` file. I'll create this in the `backend/processing` directory:

```python:backend/processing/analyze_md_tokens.py
#!/usr/bin/env python3
"""
Script to analyze token counts for each line in a German OCR-parsed .md file.
Uses the local vLLM tokenization endpoint to count tokens per line.
"""

import sys
import os
from pathlib import Path

# Add the backend directory to the Python path so we can import local_call
sys.path.append(str(Path(__file__).parent.parent))

from utils.local_call import get_token_count_from_api

# --- Configuration ---
MD_FILE_PATH = "/path/to/your/file.md"  # SPECIFY YOUR .MD FILE PATH HERE
MODEL_ID = "your-model-id"  # SPECIFY YOUR MODEL ID HERE
TOKEN_THRESHOLD = 1024
# --- ---

def analyze_md_file_tokens(file_path: str, model_id: str, threshold: int = 1024):
    """
    Analyze token counts for each line in an .md file.
    
    Args:
        file_path: Path to the .md file
        model_id: Model identifier for tokenization
        threshold: Token count threshold to flag lines
    """
    if not os.path.exists(file_path):
        print(f"Error: File {file_path} does not exist.")
        return
    
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            lines = file.readlines()
    except Exception as e:
        print(f"Error reading file {file_path}: {e}")
        return
    
    print(f"Analyzing {len(lines)} lines from: {file_path}")
    print(f"Using model: {model_id}")
    print(f"Token threshold: {threshold}")
    print("-" * 50)
    
    over_threshold_lines = []
    line_token_counts = []
    
    for line_num, line in enumerate(lines, 1):
        # Skip empty lines
        line_content = line.strip()
        if not line_content:
            continue
            
        # Get token count for this line
        token_count = get_token_count_from_api(model_id, line_content)
        
        if token_count is None:
            print(f"Warning: Could not get token count for line {line_num}")
            continue
        
        line_token_counts.append((line_num, token_count, line_content))
        
        # Check if line exceeds threshold
        if token_count > threshold:
            over_threshold_lines.append((line_num, token_count, line_content))
        
        # Progress indicator
        if line_num % 100 == 0:
            print(f"Processed {line_num} lines...")
    
    # Results
    print(f"\nAnalysis complete!")
    print(f"Total lines processed: {len(line_token_counts)}")
    print(f"Lines with more than {threshold} tokens: {len(over_threshold_lines)}")
    
    if over_threshold_lines:
        print(f"\nLines exceeding {threshold} tokens:")
        for line_num, token_count, content in over_threshold_lines:
            preview = content[:100] + "..." if len(content) > 100 else content
            print(f"  Line {line_num}: {token_count} tokens - {preview}")
    
    # Top 5 lines with most tokens
    print(f"\nTop 5 lines with highest token counts:")
    sorted_lines = sorted(line_token_counts, key=lambda x: x[1], reverse=True)
    for i, (line_num, token_count, content) in enumerate(sorted_lines[:5], 1):
        preview = content[:100] + "..." if len(content) > 100 else content
        print(f"  {i}. Line {line_num}: {token_count} tokens - {preview}")

def main():
    if not os.path.exists(MD_FILE_PATH):
        print(f"Please update MD_FILE_PATH in the script to point to your .md file.")
        print(f"Current path: {MD_FILE_PATH}")
        return
    
    print("Starting token analysis for German OCR text...")
    analyze_md_file_tokens(MD_FILE_PATH, MODEL_ID, TOKEN_THRESHOLD)

if __name__ == "__main__":
    main()
```

To use this script:

1. **Update the configuration section** at the top:
   - Set `MD_FILE_PATH` to the path of your `.md` file
   - Set `MODEL_ID` to your model identifier (the same one you use with your vLLM setup)

2. **Run the script** from your project root:
   ```bash
   cd /home/lukas/projects/pdf_mindmap
   python backend/processing/analyze_md_tokens.py
   ```

The script will:
- Read your `.md` file line by line
- Use your `get_token_count_from_api` function to get token counts
- Count how many lines exceed 1024 tokens
- Show you the line numbers and previews of lines that exceed the threshold
- Display the top 5 lines with the highest token counts
- Show progress as it processes the file

This will help you understand if your small LLM can handle the text chunks and identify problematic lines that might need to be split or processed differently.

---

_**User**_

thanks, make it use this function from local_call.py for the model id. also I renamed it to md_token.py so it is a shorter name

---

_**Assistant**_

Perfect! I'll update the script to use the `get_first_model_name` function and create it with the shorter name `md_token.py`:

```python:backend/processing/md_token.py
#!/usr/bin/env python3
"""
Script to analyze token counts for each line in a German OCR-parsed .md file.
Uses the local vLLM tokenization endpoint to count tokens per line.
"""

import sys
import os
from pathlib import Path

# Add the backend directory to the Python path so we can import local_call
sys.path.append(str(Path(__file__).parent.parent))

from utils.local_call import get_token_count_from_api, get_first_model_name

# --- Configuration ---
MD_FILE_PATH = "/path/to/your/file.md"  # SPECIFY YOUR .MD FILE PATH HERE
MODELS_ENDPOINT = "http://localhost:8000/v1/models"  # UPDATE IF YOUR ENDPOINT IS DIFFERENT
TOKEN_THRESHOLD = 1024
# --- ---

def analyze_md_file_tokens(file_path: str, model_id: str, threshold: int = 1024):
    """
    Analyze token counts for each line in an .md file.
    
    Args:
        file_path: Path to the .md file
        model_id: Model identifier for tokenization
        threshold: Token count threshold to flag lines
    """
    if not os.path.exists(file_path):
        print(f"Error: File {file_path} does not exist.")
        return
    
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            lines = file.readlines()
    except Exception as e:
        print(f"Error reading file {file_path}: {e}")
        return
    
    print(f"Analyzing {len(lines)} lines from: {file_path}")
    print(f"Using model: {model_id}")
    print(f"Token threshold: {threshold}")
    print("-" * 50)
    
    over_threshold_lines = []
    line_token_counts = []
    
    for line_num, line in enumerate(lines, 1):
        # Skip empty lines
        line_content = line.strip()
        if not line_content:
            continue
            
        # Get token count for this line
        token_count = get_token_count_from_api(model_id, line_content)
        
        if token_count is None:
            print(f"Warning: Could not get token count for line {line_num}")
            continue
        
        line_token_counts.append((line_num, token_count, line_content))
        
        # Check if line exceeds threshold
        if token_count > threshold:
            over_threshold_lines.append((line_num, token_count, line_content))
        
        # Progress indicator
        if line_num % 100 == 0:
            print(f"Processed {line_num} lines...")
    
    # Results
    print(f"\nAnalysis complete!")
    print(f"Total lines processed: {len(line_token_counts)}")
    print(f"Lines with more than {threshold} tokens: {len(over_threshold_lines)}")
    
    if over_threshold_lines:
        print(f"\nLines exceeding {threshold} tokens:")
        for line_num, token_count, content in over_threshold_lines:
            preview = content[:100] + "..." if len(content) > 100 else content
            print(f"  Line {line_num}: {token_count} tokens - {preview}")
    
    # Top 5 lines with most tokens
    print(f"\nTop 5 lines with highest token counts:")
    sorted_lines = sorted(line_token_counts, key=lambda x: x[1], reverse=True)
    for i, (line_num, token_count, content) in enumerate(sorted_lines[:5], 1):
        preview = content[:100] + "..." if len(content) > 100 else content
        print(f"  {i}. Line {line_num}: {token_count} tokens - {preview}")

def main():
    if not os.path.exists(MD_FILE_PATH):
        print(f"Please update MD_FILE_PATH in the script to point to your .md file.")
        print(f"Current path: {MD_FILE_PATH}")
        return
    
    # Get the first available model
    print("Fetching available model...")
    model_id = get_first_model_name(MODELS_ENDPOINT)
    if not model_id:
        print("Error: Could not get model ID from the server.")
        return
    
    print("Starting token analysis for German OCR text...")
    analyze_md_file_tokens(MD_FILE_PATH, model_id, TOKEN_THRESHOLD)

if __name__ == "__main__":
    main()
```

The key changes:
1. **Imported** `get_first_model_name` from `local_call.py`
2. **Added** `MODELS_ENDPOINT` configuration variable
3. **Removed** the manual `MODEL_ID` configuration
4. **Updated** `main()` to automatically fetch the first available model using `get_first_model_name()`

Now you just need to:
1. Set `MD_FILE_PATH` to your `.md` file path
2. Update `MODELS_ENDPOINT` if your vLLM server is running on a different address/port
3. Run: `python backend/processing/md_token.py`

The script will automatically discover and use the first available model from your vLLM server!

---

_**User**_

how can I debug this error? make a curl request first? what does this mean. Also I checked the tokenize endpoint. Furthermore I am running this script and commands from the linux ubuntu where the model is hosted so we can use localhost

---

_**Assistant**_

The error shows that the connection is being refused when trying to reach `192.168.178.61:8000`, but you mentioned the model is hosted locally, so let's debug this step by step.

First, let's update your script to use `localhost` and debug the connection:

```python:backend/processing/md_token.py
# ... existing code ...

# --- Configuration ---
MD_FILE_PATH = "/home/lukas/projects/pdf_mindmap/backend/processing/marker_output/ZPO.md"  # SPECIFY YOUR .MD FILE PATH HERE
VLLM_API_BASE = "http://localhost:8000"  # Changed from 192.168.178.61 to localhost
MODELS_ENDPOINT = f"{VLLM_API_BASE}/v1/models"  # Fixed: removed the replace logic
TOKEN_THRESHOLD = 1024
# --- ---

# ... existing code ...
```

Now let's debug the tokenize endpoint. The error `{"detail":"Method Not Allowed"}` suggests you're making a GET request to an endpoint that expects POST. Let's test the correct endpoints:

**1. First, check if your vLLM server is running and what endpoints are available:**
```bash
# Check if the server is running
curl http://localhost:8000/health

# Check available models
curl http://localhost:8000/v1/models

# Check if there's an OpenAPI docs endpoint
curl http://localhost:8000/docs
# or
curl http://localhost:8000/openapi.json
```

**2. Test the tokenize endpoint with the correct method (POST):**
```bash
# Test the tokenize endpoint with POST
curl -X POST http://localhost:8000/tokenize \
  -H "Content-Type: application/json" \
  -d '{
    "model": "your-model-name",
    "prompt": "Hello world",
    "add_special_tokens": true
  }'
```

**3. Also test the v1 version of tokenize:**
```bash
# Some vLLM setups use the v1 prefix
curl -X POST http://localhost:8000/v1/tokenize \
  -H "Content-Type: application/json" \
  -d '{
    "model": "your-model-name", 
    "prompt": "Hello world",
    "add_special_tokens": true
  }'
```

The issue might be:

1. **Wrong endpoint path**: Your `local_call.py` might be using the wrong tokenize endpoint URL
2. **vLLM version differences**: Different vLLM versions might have the tokenize endpoint at different paths
3. **Server not running**: The vLLM server might not be running on port 8000

Can you check what the `TOKENIZE_ENDPOINT` variable is set to in your `local_call.py` file? And run these curl commands to see which one works?

Also, you can check what ports are actually listening:
```bash
# Check what's listening on port 8000
netstat -tlnp | grep 8000
# or
ss -tlnp | grep 8000
```

Let me know the results of these tests and I can help you fix the endpoint configuration!

---

_**User**_

thanks, I had to add localhost somewhere else. Anyways, can you now please write a second script, md_process.py that uses this function to make calls to the llm (we can do batch calls, what is the ideal setup here?). Also get theI will specify a prompt with which to process the text, then write the result back into a new .md file. Only process lines that are not empty.

---

_**Assistant**_

I'll create a script that processes the markdown file using your LLM function. For batch processing, we have a few options - let me create a script that processes line by line but with some optimizations:

```python:backend/processing/md_process.py
#!/usr/bin/env python3
"""
Script to process German OCR-parsed .md file using local vLLM.
Processes each non-empty line with a specified prompt and writes results to a new file.
"""

import sys
import os
import time
from pathlib import Path

# Add the backend directory to the Python path so we can import local_call
sys.path.append(str(Path(__file__).parent.parent))

from utils.local_call import call_local_vllm

# --- Configuration ---
INPUT_MD_FILE = "/home/lukas/projects/pdf_mindmap/backend/processing/marker_output/ZPO.md"
OUTPUT_MD_FILE = "/home/lukas/projects/pdf_mindmap/backend/processing/marker_output/ZPO_processed.md"
PROCESSING_PROMPT = """Please correct any OCR errors and properly separate compound German words with appropriate spacing. Maintain the original meaning and structure. Return only the corrected text without any explanations."""
BATCH_SIZE = 5  # Process this many lines before writing to file (for progress saving)
DELAY_BETWEEN_CALLS = 0.5  # Seconds to wait between API calls to avoid overwhelming the server
# --- ---

def process_md_file(input_path: str, output_path: str, prompt_template: str, batch_size: int = 5, delay: float = 0.5):
    """
    Process each non-empty line of a markdown file using the local LLM.
    
    Args:
        input_path: Path to the input .md file
        output_path: Path to save the processed .md file
        prompt_template: The prompt to use for processing each line
        batch_size: Number of lines to process before saving progress
        delay: Seconds to wait between API calls
    """
    if not os.path.exists(input_path):
        print(f"Error: Input file {input_path} does not exist.")
        return
    
    try:
        with open(input_path, 'r', encoding='utf-8') as file:
            lines = file.readlines()
    except Exception as e:
        print(f"Error reading input file {input_path}: {e}")
        return
    
    print(f"Processing {len(lines)} lines from: {input_path}")
    print(f"Output will be saved to: {output_path}")
    print(f"Batch size: {batch_size}, Delay between calls: {delay}s")
    print(f"Prompt: {prompt_template}")
    print("-" * 50)
    
    processed_lines = []
    non_empty_count = 0
    processed_count = 0
    failed_count = 0
    
    # Create output directory if it doesn't exist
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    try:
        for line_num, line in enumerate(lines, 1):
            line_content = line.strip()
            
            # Keep empty lines as-is
            if not line_content:
                processed_lines.append(line)
                continue
            
            non_empty_count += 1
            print(f"\nProcessing line {line_num} (non-empty #{non_empty_count}): {line_content[:100]}...")
            
            # Create the full prompt
            full_prompt = f"{prompt_template}\n\nText to process: {line_content}"
            
            # Call the LLM
            result = call_local_vllm(full_prompt)
            
            if result:
                # Add the processed result with original line ending
                processed_lines.append(result.strip() + line[len(line.rstrip()):])  # Preserve original line ending
                processed_count += 1
                print(f"â Successfully processed line {line_num}")
            else:
                # Keep original line if processing failed
                processed_lines.append(line)
                failed_count += 1
                print(f"â Failed to process line {line_num}, keeping original")
            
            # Add delay between calls to avoid overwhelming the server
            if delay > 0:
                time.sleep(delay)
            
            # Save progress in batches
            if processed_count % batch_size == 0:
                save_progress(processed_lines, output_path, line_num, len(lines))
            
            # Progress update
            if line_num % 10 == 0:
                print(f"\nProgress: {line_num}/{len(lines)} lines processed "
                      f"({processed_count} successful, {failed_count} failed)")
        
        # Final save
        save_final_result(processed_lines, output_path)
        
        # Summary
        print(f"\n" + "=" * 50)
        print(f"Processing complete!")
        print(f"Total lines: {len(lines)}")
        print(f"Non-empty lines: {non_empty_count}")
        print(f"Successfully processed: {processed_count}")
        print(f"Failed to process: {failed_count}")
        print(f"Output saved to: {output_path}")
        
    except KeyboardInterrupt:
        print(f"\n\nProcessing interrupted by user!")
        print(f"Saving progress up to line {line_num}...")
        save_progress(processed_lines, output_path + ".partial", line_num, len(lines))
        print(f"Partial results saved to: {output_path}.partial")
        return
    except Exception as e:
        print(f"Unexpected error during processing: {e}")
        save_progress(processed_lines, output_path + ".error", line_num if 'line_num' in locals() else 0, len(lines))
        return

def save_progress(processed_lines: list, output_path: str, current_line: int, total_lines: int):
    """Save current progress to file."""
    try:
        with open(output_path, 'w', encoding='utf-8') as file:
            file.writelines(processed_lines)
        print(f"Progress saved: {current_line}/{total_lines} lines")
    except Exception as e:
        print(f"Error saving progress: {e}")

def save_final_result(processed_lines: list, output_path: str):
    """Save final processed result to file."""
    try:
        with open(output_path, 'w', encoding='utf-8') as file:
            file.writelines(processed_lines)
        print(f"Final result saved to: {output_path}")
    except Exception as e:
        print(f"Error saving final result: {e}")

def main():
    if not os.path.exists(INPUT_MD_FILE):
        print(f"Please update INPUT_MD_FILE in the script to point to your .md file.")
        print(f"Current path: {INPUT_MD_FILE}")
        return
    
    print("Starting markdown file processing with local LLM...")
    print(f"Input: {INPUT_MD_FILE}")
    print(f"Output: {OUTPUT_MD_FILE}")
    
    # Ask for confirmation
    response = input("\nProceed with processing? (y/N): ")
    if response.lower() != 'y':
        print("Processing cancelled.")
        return
    
    process_md_file(
        INPUT_MD_FILE, 
        OUTPUT_MD_FILE, 
        PROCESSING_PROMPT, 
        BATCH_SIZE, 
        DELAY_BETWEEN_CALLS
    )

if __name__ == "__main__":
    main()
```

This script provides several features for efficient batch processing:

**Key Features:**
1. **Line-by-line processing**: Processes each non-empty line individually (better for OCR correction)
2. **Progress saving**: Saves progress every `BATCH_SIZE` lines so you don't lose work if interrupted
3. **Error handling**: Keeps original text if LLM processing fails
4. **Delay between calls**: Configurable delay to avoid overwhelming your local server
5. **Progress tracking**: Shows detailed progress and statistics
6. **Interrupt handling**: Gracefully handles Ctrl+C and saves partial results

**Configuration options:**
- `BATCH_SIZE`: How many lines to process before saving progress (5 is a good start)
- `DELAY_BETWEEN_CALLS`: Seconds between API calls (0.5s should be fine for local)
- `PROCESSING_PROMPT`: Customize this for your specific OCR correction needs

**Usage:**
```bash
python backend/processing/md_process.py
```

The script will ask for confirmation before starting and show progress as it runs. If interrupted, it saves partial results with a `.partial` extension.

Would you like me to adjust the batch processing approach or modify the prompt for better German OCR correction?

---

_**User**_

thanks, can you make sure this prompt format is used? <|system|>{system_prompt}<|end|><|user|>{prompt}<|end|><|assistant|>

---

_**Assistant**_

I'll update the script to use that specific prompt format. Let me modify the `md_process.py` file:

---

```diff
  INPUT_MD_FILE = "/home/lukas/projects/pdf_mindmap/backend/processing/marker_output/ZPO_temp/ZPO.md"
  OUTPUT_MD_FILE = "/home/lukas/projects/pdf_mindmap/backend/processing/marker_output/ZPO_temp/ZPO_processed.md"
- PROCESSING_PROMPT = """Im untenstehenden Text kÃ¶nnen Leerzeichen fehlen. Bitte fÃ¼ge Leerzeichen zwischen WÃ¶rtern ein wo nÃ¶tig. VerÃ¤ndere sonst nichts. FÃ¼ge dem Text nichts hinzu, gebe nur den korrigierten Text zurÃ¼ck."""
+ SYSTEM_PROMPT = "Du bist ein hilfreicher Assistent, der OCR-Fehler in deutschen Texten korrigiert."
+ USER_PROMPT_TEMPLATE = "Im untenstehenden Text kÃ¶nnen Leerzeichen fehlen. Bitte fÃ¼ge Leerzeichen zwischen WÃ¶rtern ein wo nÃ¶tig. VerÃ¤ndere sonst nichts. FÃ¼ge dem Text nichts hinzu, gebe nur den korrigierten Text zurÃ¼ck.\n\nBitte korrigiere den folgenden Text: {text}"
  BATCH_SIZE = 146  # Process this many lines before writing to file (for progress saving)
  DELAY_BETWEEN_CALLS = 0.01  # Seconds to wait between API calls to avoid overwhelming the server
```

```diff
          INPUT_MD_FILE, 
          OUTPUT_MD_FILE, 
-         PROCESSING_PROMPT, 
+         USER_PROMPT_TEMPLATE, 
          BATCH_SIZE, 
          DELAY_BETWEEN_CALLS
```

---

```diff
  # --- ---
  
- def process_md_file(input_path: str, output_path: str, prompt_template: str, batch_size: int = 5, delay: float = 0.01):
+ def process_md_file(input_path: str, output_path: str, user_prompt_template: str, batch_size: int = 5, delay: float = 0.01):
      """
      Process each non-empty line of a markdown file using the local LLM.
```

```diff
          input_path: Path to the input .md file
          output_path: Path to save the processed .md file
-         prompt_template: The prompt to use for processing each line
+         user_prompt_template: The user prompt template to use for processing each line
          batch_size: Number of lines to process before saving progress
          delay: Seconds to wait between API calls
```

```diff
      print(f"Output will be saved to: {output_path}")
      print(f"Batch size: {batch_size}, Delay between calls: {delay}s")
-     print(f"Prompt: {prompt_template}")
+     print(f"System prompt: {SYSTEM_PROMPT}")
+     print(f"User prompt template: {user_prompt_template}")
      print("-" * 50)
      
```

```diff
              
              # Create the full prompt
-             full_prompt = f"{prompt_template}\n\nBitte korrigiere den folgenden Text: {line_content}"
+             full_prompt = f"{user_prompt_template}\n\nBitte korrigiere den folgenden Text: {line_content}"
              
              # Call the LLM
```

---

```diff
              print(f"\nProcessing line {line_num} (non-empty #{non_empty_count}): {line_content[:100]}...")
              
-             # Create the full prompt
-             full_prompt = f"{user_prompt_template}\n\nBitte korrigiere den folgenden Text: {line_content}"
+             # Create the full prompt using the specified chat template format
+             user_prompt = user_prompt_template.format(text=line_content)
+             full_prompt = f"<|system|>{SYSTEM_PROMPT}<|end|><|user|>{user_prompt}<|end|><|assistant|>"
              
              # Call the LLM
```

---

Perfect! I've updated the script to use the specific prompt format you requested. Here are the key changes:

1. **Split the prompt into system and user components**:
   - `SYSTEM_PROMPT`: "Du bist ein hilfreicher Assistent, der OCR-Fehler in deutschen Texten korrigiert."
   - `USER_PROMPT_TEMPLATE`: Contains the instruction and uses `{text}` placeholder

2. **Updated the prompt formatting** to use your specified chat template:
   ```
   <|system|>{system_prompt}<|end|><|user|>{user_prompt}<|end|><|assistant|>
   ```

3. **Modified the processing logic** to:
   - Format the user prompt with the actual text using `.format(text=line_content)`
   - Construct the full prompt using the chat template format

Now when processing each line, the script will create prompts in this exact format:
```
<|system|>Du bist ein hilfreicher Assistent, der OCR-Fehler in deutschen Texten korrigiert.<|end|><|user|>Im untenstehenden Text kÃ¶nnen Leerzeichen fehlen. Bitte fÃ¼ge Leerzeichen zwischen WÃ¶rtern ein wo nÃ¶tig. VerÃ¤ndere sonst nichts. FÃ¼ge dem Text nichts hinzu, gebe nur den korrigierten Text zurÃ¼ck.

Bitte korrigiere den folgenden Text: [actual line content]<|end|><|assistant|>
```

This format should work properly with your local LLM that expects this specific chat template structure!

---

