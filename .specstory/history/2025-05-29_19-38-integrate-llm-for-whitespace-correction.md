<!-- Generated by SpecStory -->

# Integrate llm for whitespace correction (2025-05-29 21:38:04)

_**User**_

I have a project that uses ocr to parse a pdf. However as the text is German, white space is missing. So I want to use an llm for adding the white space to avoidtextlikethis. I have set up an openai compatible server with a model. The server can do batch processing. Here is the curl request that works with the settings I want ":null,"code":400}(vllm-env) lukas@ubuntu:~$ curl http://localhost:8000/v1/chat/completions -H "Content-Type: application/json" -d '{
  "model": "/home/lukas/projects/LLM_testing/webui/text-generation-webui-main/models/Qwen3-0.6B-FP8",
  "messages": [
    {"role": "user", "content": "Give me a short introduction to large language models."}
  ],
  "temperature": 0,
  "top_p": 0.8,
  "top_k": 20,
  "max_tokens": 2000,
  "presence_penalty": 1.5,
  "chat_template_kwargs": {"enable_thinking": false}
}'". I have a file already, local_call.py with some functions for fetching the model name dynamically and setting the max tokens based on input tokens. Please use this as inspiration, then add a new function "call_qwen3" that supports all the sampling parameters above / has them as default params (except for max tokens and model name, they get determined by the existing functions). Then update my script md_process.py to use that function and make acutall, concurrent batch calls (it is alledegly doing batch calls, but in reality it waits for each call's result). 

---

_**Assistant**_



---

I'll help you create a new function `call_qwen3` with the sampling parameters from your curl request and update the processing script to use actual concurrent batch processing.

First, let's add the new function to `local_call.py`:

```python:backend/utils/local_call.py
# ... existing code ...

def call_qwen3(prompt: str, 
              temperature: float = 0,
              top_p: float = 0.8, 
              top_k: int = 20,
              presence_penalty: float = 1.5,
              enable_thinking: bool = False):
    """
    Makes a call to the local vLLM OpenAI-compatible server with Qwen3-specific parameters,
    dynamically fetching the model name and calculating max_tokens using the /tokenize endpoint.

    Args:
        prompt: The user prompt to send to the model.
        temperature: Controls randomness in generation (0 = deterministic).
        top_p: Nucleus sampling parameter.
        top_k: Top-k sampling parameter.
        presence_penalty: Penalty for tokens that have appeared in the text so far.
        enable_thinking: Whether to enable thinking mode in chat template.

    Returns:
        The content of the model's response, or None if an error occurs.
    """
    # --- Get Model Name Dynamically ---
    model_name = get_first_model_name(MODELS_ENDPOINT)
    if not model_name:
        print("Exiting as model name could not be determined.")
        return None
    # --- ---

    # --- Prepare Messages and Calculate Input Tokens using API ---
    system_message = {"role": "system", "content": "Du bist ein hilfreicher Assistent und Experte im Korrigieren von Text."}
    user_message = {"role": "user", "content": prompt}
    messages = [system_message, user_message]

    # Combine message content for tokenization
    text_for_tokenization = "\n".join([msg["content"] for msg in messages])

    input_tokens = get_token_count_from_api(model_name, text_for_tokenization)

    if input_tokens is not None:
        print(f"--- Input Token Count (from API): {input_tokens} ---")
    else:
        print("Warning: Failed to get token count from API. Cannot calculate max_tokens accurately.")
        input_tokens = -1

    # --- Calculate max_tokens ---
    if input_tokens != -1 and input_tokens < MODEL_CONTEXT_LIMIT:
        max_response_tokens = MODEL_CONTEXT_LIMIT - input_tokens - TOKEN_BUFFER
        if max_response_tokens <= 0:
            print(f"Error: Input prompt ({input_tokens} tokens) is too long for the model's context limit ({MODEL_CONTEXT_LIMIT} tokens). Cannot generate response.")
            return None
        print(f"--- Max Response Tokens Calculated: {max_response_tokens} ---")
    else:
        print("Warning: Using fallback max_tokens value (2000).")
        max_response_tokens = 2000
        if input_tokens >= MODEL_CONTEXT_LIMIT:
             print(f"Error: Input prompt ({input_tokens} tokens) likely exceeds model context limit ({MODEL_CONTEXT_LIMIT}). API call will probably fail.")
    # --- ---

    try:
        # Initialize the OpenAI client
        client = openai.OpenAI(
            api_key=API_KEY,
            base_url=VLLM_API_BASE,
        )

        print(f"--- Sending prompt to {model_name} at {VLLM_API_BASE} ---")

        # Create the chat completion request with Qwen3-specific parameters
        response = client.chat.completions.create(
            model=model_name,
            messages=messages,
            max_tokens=max_response_tokens,
            temperature=temperature,
            top_p=top_p,
            top_k=top_k,
            presence_penalty=presence_penalty,
            extra_body={
                "chat_template_kwargs": {"enable_thinking": enable_thinking}
            }
        )

        # Extract the response content
        if response.choices:
            message_content = response.choices[0].message.content
            print("\n--- Model Response ---")
            print(message_content)
            if response.usage:
                 output_tokens = response.usage.completion_tokens
                 print(f"Reported Output Tokens: {output_tokens}")
            print("--------------------")
            return message_content
        else:
            print("Error: No response choices received.")
            return None

    except openai.APIConnectionError as e:
        print(f"Connection Error: Failed to connect to the vLLM server at {VLLM_API_BASE}.")
        print(f"Details: {e}")
        return None
    except openai.APIError as e:
        print(f"API Error: An error occurred during the API call.")
        print(f"HTTP Status: {e.status_code}")
        try:
            response_text = e.response.text
            if "context_length_exceeded" in response_text.lower():
                 print("Error Details: The request likely exceeded the model's maximum context length.")
                 print(f"Input Tokens: {input_tokens if input_tokens != -1 else 'Error calculating'}, Requested Max Output Tokens: {max_response_tokens}, Total Limit: {MODEL_CONTEXT_LIMIT}")
            print(f"Response Body: {response_text}")
        except Exception:
             print(f"Response Body: Could not decode response body.")
        return None
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return None

# ... existing code ...
```

Now let's update `md_process.py` to use concurrent batch processing:

```python:backend/processing/md_process.py
#!/usr/bin/env python3
"""
Script to process German OCR-parsed .md file using local vLLM with concurrent batch processing.
Processes lines in batches with concurrent API calls and writes results to a new file.
"""

import sys
import os
import time
import asyncio
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Tuple, Optional

# Add the backend directory to the Python path so we can import local_call
sys.path.append(str(Path(__file__).parent.parent))

from utils.local_call import call_qwen3

# --- Configuration ---
INPUT_MD_FILE = "/home/lukas/projects/pdf_mindmap/backend/processing/marker_output/ZPO_temp/ZPO.md"
OUTPUT_MD_FILE = "/home/lukas/projects/pdf_mindmap/backend/processing/marker_output/ZPO_temp/ZPO_processed.md"
USER_PROMPT_TEMPLATE = "Im untenstehenden Text können Leerzeichen fehlen. Bitte füge Leerzeichen zwischen Wörtern ein wo nötig. Verändere sonst nichts. Füge dem Text nichts hinzu, gebe nur den korrigierten Text zurück.\n\nBitte korrigiere den folgenden Text: {text}"
BATCH_SIZE = 10  # Number of concurrent API calls to make
MAX_WORKERS = 5  # Maximum number of concurrent threads
PROGRESS_SAVE_INTERVAL = 50  # Save progress every N processed lines
# --- ---

def process_single_line(line_data: Tuple[int, str]) -> Tuple[int, str, bool]:
    """
    Process a single line using the LLM.
    
    Args:
        line_data: Tuple of (line_number, line_content)
        
    Returns:
        Tuple of (line_number, processed_content, success_flag)
    """
    line_num, line_content = line_data
    
    if not line_content.strip():
        return line_num, line_content, True  # Keep empty lines as-is
    
    print(f"Processing line {line_num}: {line_content.strip()[:100]}...")
    
    # Create the prompt
    user_prompt = USER_PROMPT_TEMPLATE.format(text=line_content.strip())
    
    try:
        # Call the LLM with Qwen3 parameters
        result = call_qwen3(
            prompt=user_prompt,
            temperature=0,
            top_p=0.8,
            top_k=20,
            presence_penalty=1.5,
            enable_thinking=False
        )
        
        if result:
            # Preserve original line ending
            processed_content = result.strip() + line_content[len(line_content.rstrip()):]
            print(f"✓ Successfully processed line {line_num}")
            return line_num, processed_content, True
        else:
            print(f"✗ Failed to process line {line_num}, keeping original")
            return line_num, line_content, False
            
    except Exception as e:
        print(f"✗ Error processing line {line_num}: {e}")
        return line_num, line_content, False

def process_batch_concurrent(lines_batch: List[Tuple[int, str]], max_workers: int) -> List[Tuple[int, str, bool]]:
    """
    Process a batch of lines concurrently.
    
    Args:
        lines_batch: List of (line_number, line_content) tuples
        max_workers: Maximum number of concurrent threads
        
    Returns:
        List of (line_number, processed_content, success_flag) tuples
    """
    results = []
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Submit all tasks
        future_to_line = {
            executor.submit(process_single_line, line_data): line_data 
            for line_data in lines_batch
        }
        
        # Collect results as they complete
        for future in as_completed(future_to_line):
            try:
                result = future.result()
                results.append(result)
            except Exception as e:
                line_data = future_to_line[future]
                print(f"Exception in thread for line {line_data[0]}: {e}")
                # Return original line on exception
                results.append((line_data[0], line_data[1], False))
    
    # Sort results by line number to maintain order
    results.sort(key=lambda x: x[0])
    return results

def process_md_file_concurrent(input_path: str, output_path: str, user_prompt_template: str, 
                             batch_size: int = 10, max_workers: int = 5, progress_save_interval: int = 50):
    """
    Process each non-empty line of a markdown file using concurrent LLM calls.
    
    Args:
        input_path: Path to the input .md file
        output_path: Path to save the processed .md file
        user_prompt_template: The user prompt template to use for processing each line
        batch_size: Number of lines to process concurrently in each batch
        max_workers: Maximum number of concurrent threads
        progress_save_interval: Save progress every N processed lines
    """
    if not os.path.exists(input_path):
        print(f"Error: Input file {input_path} does not exist.")
        return
    
    try:
        with open(input_path, 'r', encoding='utf-8') as file:
            lines = file.readlines()
    except Exception as e:
        print(f"Error reading input file {input_path}: {e}")
        return
    
    print(f"Processing {len(lines)} lines from: {input_path}")
    print(f"Output will be saved to: {output_path}")
    print(f"Batch size: {batch_size}, Max workers: {max_workers}")
    print(f"Progress save interval: {progress_save_interval}")
    print("-" * 50)
    
    # Create output directory if it doesn't exist
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    # Prepare line data with line numbers
    lines_with_numbers = [(i + 1, line) for i, line in enumerate(lines)]
    
    # Filter out empty lines for processing, but keep track of all lines
    non_empty_lines = [(num, line) for num, line in lines_with_numbers if line.strip()]
    
    print(f"Found {len(non_empty_lines)} non-empty lines to process")
    
    processed_results = {}  # line_number -> (processed_content, success_flag)
    total_processed = 0
    total_failed = 0
    
    try:
        # Process in batches
        for i in range(0, len(non_empty_lines), batch_size):
            batch = non_empty_lines[i:i + batch_size]
            batch_start = i + 1
            batch_end = min(i + batch_size, len(non_empty_lines))
            
            print(f"\n--- Processing batch {batch_start}-{batch_end} of {len(non_empty_lines)} ---")
            start_time = time.time()
            
            # Process batch concurrently
            batch_results = process_batch_concurrent(batch, max_workers)
            
            # Store results
            for line_num, processed_content, success in batch_results:
                processed_results[line_num] = (processed_content, success)
                if success:
                    total_processed += 1
                else:
                    total_failed += 1
            
            batch_time = time.time() - start_time
            print(f"Batch completed in {batch_time:.2f}s")
            print(f"Progress: {total_processed} successful, {total_failed} failed")
            
            # Save progress periodically
            if total_processed % progress_save_interval == 0:
                save_progress_concurrent(lines_with_numbers, processed_results, output_path, total_processed + total_failed, len(lines))
        
        # Reconstruct the final output maintaining original line order
        final_lines = []
        for line_num, original_line in lines_with_numbers:
            if line_num in processed_results:
                processed_content, _ = processed_results[line_num]
                final_lines.append(processed_content)
            else:
                final_lines.append(original_line)  # Keep original for empty lines
        
        # Save final result
        save_final_result(final_lines, output_path)
        
        # Summary
        print(f"\n" + "=" * 50)
        print(f"Processing complete!")
        print(f"Total lines: {len(lines)}")
        print(f"Non-empty lines: {len(non_empty_lines)}")
        print(f"Successfully processed: {total_processed}")
        print(f"Failed to process: {total_failed}")
        print(f"Output saved to: {output_path}")
        
    except KeyboardInterrupt:
        print(f"\n\nProcessing interrupted by user!")
        print(f"Saving progress...")
        # Reconstruct partial output
        partial_lines = []
        for line_num, original_line in lines_with_numbers:
            if line_num in processed_results:
                processed_content, _ = processed_results[line_num]
                partial_lines.append(processed_content)
            else:
                partial_lines.append(original_line)
        save_final_result(partial_lines, output_path + ".partial")
        print(f"Partial results saved to: {output_path}.partial")
        return
    except Exception as e:
        print(f"Unexpected error during processing: {e}")
        return

def save_progress_concurrent(all_lines: List[Tuple[int, str]], processed_results: dict, 
                           output_path: str, current_count: int, total_lines: int):
    """Save current progress to file."""
    try:
        lines_to_save = []
        for line_num, original_line in all_lines:
            if line_num in processed_results:
                processed_content, _ = processed_results[line_num]
                lines_to_save.append(processed_content)
            else:
                lines_to_save.append(original_line)
        
        with open(output_path + ".progress", 'w', encoding='utf-8') as file:
            file.writelines(lines_to_save)
        print(f"Progress saved: {current_count} lines processed out of {total_lines}")
    except Exception as e:
        print(f"Error saving progress: {e}")

# ... existing save_final_result function ...

def main():
    if not os.path.exists(INPUT_MD_FILE):
        print(f"Please update INPUT_MD_FILE in the script to point to your .md file.")
        print(f"Current path: {INPUT_MD_FILE}")
        return
    
    print("Starting concurrent markdown file processing with local LLM...")
    print(f"Input: {INPUT_MD_FILE}")
    print(f"Output: {OUTPUT_MD_FILE}")
    print(f"Concurrent batch size: {BATCH_SIZE}")
    print(f"Max workers: {MAX_WORKERS}")
    
    # Ask for confirmation
    response = input("\nProceed with processing? (y/N): ")
    if response.lower() != 'y':
        print("Processing cancelled.")
        return
    
    process_md_file_concurrent(
        INPUT_MD_FILE, 
        OUTPUT_MD_FILE, 
        USER_PROMPT_TEMPLATE, 
        BATCH_SIZE,
        MAX_WORKERS,
        PROGRESS_SAVE_INTERVAL
    )

if __name__ == "__main__":
    main()
```

## Key Changes Made:

### In `local_call.py`:
1. **Added `call_qwen3` function** with all the sampling parameters from your curl request as default parameters
2. **Used `extra_body`** to pass `chat_template_kwargs` parameter to the API
3. **Maintained existing token calculation logic** for max_tokens and model name fetching

### In `md_process.py`:
1. **Implemented true concurrent processing** using `ThreadPoolExecutor`
2. **Batch processing** with configurable batch size and max workers
3. **Maintained line order** in the output by tracking line numbers
4. **Progress tracking and saving** during concurrent execution
5. **Error handling** for individual thread failures
6. **Updated configuration** with concurrent processing parameters

The new implementation will:
- Process multiple lines simultaneously (up to `MAX_WORKERS` threads)
- Use the Qwen3-specific parameters (temperature=0, top_p=0.8, etc.)
- Maintain the original line order in the output
- Provide better progress tracking and error handling
- Save progress periodically during processing

You can adjust `BATCH_SIZE` and `MAX_WORKERS` to optimize performance based on your server's capabilities and the model's processing speed.

---

_**User**_

thanks, can we not group lines and set them at once? Do we need the workers?

---

_**Assistant**_

You're absolutely right! If your vLLM server supports batch processing (which it sounds like it does), we can send multiple prompts in a single API call instead of using workers and concurrent requests. This would be much more efficient.

Let me update the code to use proper batch processing:

```python:backend/utils/local_call.py
# ... existing code ...

def call_qwen3_batch(prompts: List[str], 
                    temperature: float = 0,
                    top_p: float = 0.8, 
                    top_k: int = 20,
                    presence_penalty: float = 1.5,
                    enable_thinking: bool = False) -> List[str]:
    """
    Makes a batch call to the local vLLM OpenAI-compatible server with Qwen3-specific parameters.
    
    Args:
        prompts: List of user prompts to send to the model.
        temperature: Controls randomness in generation (0 = deterministic).
        top_p: Nucleus sampling parameter.
        top_k: Top-k sampling parameter.
        presence_penalty: Penalty for tokens that have appeared in the text so far.
        enable_thinking: Whether to enable thinking mode in chat template.

    Returns:
        List of response contents, or empty list if an error occurs.
    """
    # --- Get Model Name Dynamically ---
    model_name = get_first_model_name(MODELS_ENDPOINT)
    if not model_name:
        print("Exiting as model name could not be determined.")
        return []
    # --- ---

    try:
        # Initialize the OpenAI client
        client = openai.OpenAI(
            api_key=API_KEY,
            base_url=VLLM_API_BASE,
        )

        print(f"--- Sending batch of {len(prompts)} prompts to {model_name} at {VLLM_API_BASE} ---")

        # Prepare all messages for batch processing
        all_messages = []
        for prompt in prompts:
            messages = [
                {"role": "system", "content": "Du bist ein hilfreicher Assistent und Experte im Korrigieren von Text."},
                {"role": "user", "content": prompt}
            ]
            all_messages.append(messages)

        # For vLLM batch processing, we can send multiple requests in the messages field
        # or use the batch endpoint if available. Let's try the batch approach first.
        
        # If your vLLM server supports true batch processing, use this format:
        batch_data = {
            "model": model_name,
            "messages": all_messages,  # Send all message sets
            "max_tokens": 2000,  # You might want to calculate this based on input
            "temperature": temperature,
            "top_p": top_p,
            "top_k": top_k,
            "presence_penalty": presence_penalty,
            "extra_body": {
                "chat_template_kwargs": {"enable_thinking": enable_thinking}
            }
        }

        # Make the batch request using requests directly since OpenAI client might not support custom batch format
        import requests
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {API_KEY}"
        }
        
        response = requests.post(
            f"{VLLM_API_BASE}/chat/completions",
            headers=headers,
            json=batch_data,
            timeout=300  # Longer timeout for batch processing
        )
        
        response.raise_for_status()
        result = response.json()

        # Extract responses - format may vary depending on your vLLM server implementation
        responses = []
        if "choices" in result:
            # If the server returns multiple choices for batch
            for choice in result["choices"]:
                if choice.get("message", {}).get("content"):
                    responses.append(choice["message"]["content"])
                else:
                    responses.append("")  # Empty response for failed items
        else:
            print(f"Unexpected batch response format: {result}")
            return []

        print(f"--- Received {len(responses)} batch responses ---")
        return responses

    except Exception as e:
        print(f"Batch API Error: {e}")
        # Fallback to individual calls if batch fails
        print("Falling back to individual calls...")
        return [call_qwen3(prompt, temperature, top_p, top_k, presence_penalty, enable_thinking) or "" for prompt in prompts]

# ... existing code ...
```

Now let's simplify the `md_process.py` to use batch processing without workers:

```python:backend/processing/md_process.py
#!/usr/bin/env python3
"""
Script to process German OCR-parsed .md file using local vLLM with batch processing.
Processes lines in batches with single API calls.
"""

import sys
import os
import time
from pathlib import Path
from typing import List, Tuple

# Add the backend directory to the Python path so we can import local_call
sys.path.append(str(Path(__file__).parent.parent))

from utils.local_call import call_qwen3_batch, call_qwen3

# --- Configuration ---
INPUT_MD_FILE = "/home/lukas/projects/pdf_mindmap/backend/processing/marker_output/ZPO_temp/ZPO.md"
OUTPUT_MD_FILE = "/home/lukas/projects/pdf_mindmap/backend/processing/marker_output/ZPO_temp/ZPO_processed.md"
USER_PROMPT_TEMPLATE = "Im untenstehenden Text können Leerzeichen fehlen. Bitte füge Leerzeichen zwischen Wörtern ein wo nötig. Verändere sonst nichts. Füge dem Text nichts hinzu, gebe nur den korrigierten Text zurück.\n\nBitte korrigiere den folgenden Text: {text}"
BATCH_SIZE = 20  # Number of lines to process in each batch call
PROGRESS_SAVE_INTERVAL = 100  # Save progress every N processed lines
# --- ---

def process_md_file_batch(input_path: str, output_path: str, user_prompt_template: str, 
                         batch_size: int = 20, progress_save_interval: int = 100):
    """
    Process markdown file using batch API calls.
    
    Args:
        input_path: Path to the input .md file
        output_path: Path to save the processed .md file
        user_prompt_template: The user prompt template to use for processing each line
        batch_size: Number of lines to process in each batch
        progress_save_interval: Save progress every N processed lines
    """
    if not os.path.exists(input_path):
        print(f"Error: Input file {input_path} does not exist.")
        return
    
    try:
        with open(input_path, 'r', encoding='utf-8') as file:
            lines = file.readlines()
    except Exception as e:
        print(f"Error reading input file {input_path}: {e}")
        return
    
    print(f"Processing {len(lines)} lines from: {input_path}")
    print(f"Output will be saved to: {output_path}")
    print(f"Batch size: {batch_size}")
    print(f"Progress save interval: {progress_save_interval}")
    print("-" * 50)
    
    # Create output directory if it doesn't exist
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    # Separate empty and non-empty lines
    line_data = []  # (line_number, line_content, is_empty)
    non_empty_indices = []  # Track which lines need processing
    
    for i, line in enumerate(lines):
        line_data.append((i + 1, line, not line.strip()))
        if line.strip():  # Non-empty line
            non_empty_indices.append(i)
    
    print(f"Found {len(non_empty_indices)} non-empty lines to process")
    
    # Initialize results array
    processed_lines = lines.copy()  # Start with original lines
    total_processed = 0
    total_failed = 0
    
    try:
        # Process non-empty lines in batches
        for batch_start in range(0, len(non_empty_indices), batch_size):
            batch_end = min(batch_start + batch_size, len(non_empty_indices))
            batch_indices = non_empty_indices[batch_start:batch_end]
            
            print(f"\n--- Processing batch {batch_start//batch_size + 1}: lines {batch_indices[0]+1}-{batch_indices[-1]+1} ---")
            start_time = time.time()
            
            # Prepare batch prompts
            batch_prompts = []
            batch_original_lines = []
            
            for idx in batch_indices:
                line_content = lines[idx].strip()
                prompt = user_prompt_template.format(text=line_content)
                batch_prompts.append(prompt)
                batch_original_lines.append(lines[idx])
            
            # Make batch API call
            try:
                batch_responses = call_qwen3_batch(
                    batch_prompts,
                    temperature=0,
                    top_p=0.8,
                    top_k=20,
                    presence_penalty=1.5,
                    enable_thinking=False
                )
                
                # Process responses
                if len(batch_responses) == len(batch_prompts):
                    for i, (idx, response) in enumerate(zip(batch_indices, batch_responses)):
                        if response and response.strip():
                            # Preserve original line ending
                            original_line = batch_original_lines[i]
                            processed_lines[idx] = response.strip() + original_line[len(original_line.rstrip()):]
                            total_processed += 1
                            print(f"✓ Processed line {idx + 1}")
                        else:
                            total_failed += 1
                            print(f"✗ Failed line {idx + 1}, keeping original")
                else:
                    print(f"Warning: Expected {len(batch_prompts)} responses, got {len(batch_responses)}")
                    total_failed += len(batch_prompts)
                    
            except Exception as e:
                print(f"Batch processing failed: {e}")
                print("Falling back to individual processing for this batch...")
                
                # Fallback to individual calls
                for i, (idx, prompt) in enumerate(zip(batch_indices, batch_prompts)):
                    try:
                        response = call_qwen3(
                            prompt,
                            temperature=0,
                            top_p=0.8,
                            top_k=20,
                            presence_penalty=1.5,
                            enable_thinking=False
                        )
                        
                        if response and response.strip():
                            original_line = batch_original_lines[i]
                            processed_lines[idx] = response.strip() + original_line[len(original_line.rstrip()):]
                            total_processed += 1
                            print(f"✓ Processed line {idx + 1} (individual)")
                        else:
                            total_failed += 1
                            print(f"✗ Failed line {idx + 1}, keeping original")
                    except Exception as e2:
                        print(f"Individual processing failed for line {idx + 1}: {e2}")
                        total_failed += 1
            
            batch_time = time.time() - start_time
            print(f"Batch completed in {batch_time:.2f}s")
            print(f"Progress: {total_processed} successful, {total_failed} failed")
            
            # Save progress periodically
            if total_processed % progress_save_interval == 0:
                save_progress_simple(processed_lines, output_path, total_processed + total_failed, len(non_empty_indices))
        
        # Save final result
        save_final_result(processed_lines, output_path)
        
        # Summary
        print(f"\n" + "=" * 50)
        print(f"Processing complete!")
        print(f"Total lines: {len(lines)}")
        print(f"Non-empty lines: {len(non_empty_indices)}")
        print(f"Successfully processed: {total_processed}")
        print(f"Failed to process: {total_failed}")
        print(f"Output saved to: {output_path}")
        
    except KeyboardInterrupt:
        print(f"\n\nProcessing interrupted by user!")
        print(f"Saving progress...")
        save_final_result(processed_lines, output_path + ".partial")
        print(f"Partial results saved to: {output_path}.partial")
        return
    except Exception as e:
        print(f"Unexpected error during processing: {e}")
        return

def save_progress_simple(processed_lines: list, output_path: str, current_count: int, total_count: int):
    """Save current progress to file."""
    try:
        with open(output_path + ".progress", 'w', encoding='utf-8') as file:
            file.writelines(processed_lines)
        print(f"Progress saved: {current_count}/{total_count} lines processed")
    except Exception as e:
        print(f"Error saving progress: {e}")

# ... existing save_final_result function ...

def main():
    if not os.path.exists(INPUT_MD_FILE):
        print(f"Please update INPUT_MD_FILE in the script to point to your .md file.")
        print(f"Current path: {INPUT_MD_FILE}")
        return
    
    print("Starting batch markdown file processing with local LLM...")
    print(f"Input: {INPUT_MD_FILE}")
    print(f"Output: {OUTPUT_MD_FILE}")
    print(f"Batch size: {BATCH_SIZE}")
    
    # Ask for confirmation
    response = input("\nProceed with processing? (y/N): ")
    if response.lower() != 'y':
        print("Processing cancelled.")
        return
    
    process_md_file_batch(
        INPUT_MD_FILE, 
        OUTPUT_MD_FILE, 
        USER_PROMPT_TEMPLATE, 
        BATCH_SIZE,
        PROGRESS_SAVE_INTERVAL
    )

if __name__ == "__main__":
    main()
```

## Key Changes:

1. **Removed ThreadPoolExecutor and workers** - No need for concurrent threads when using batch processing
2. **Added `call_qwen3_batch` function** - Sends multiple prompts in a single API call
3. **Simplified processing logic** - Process lines in batches with single API calls
4. **Fallback mechanism** - If batch processing fails, fall back to individual calls
5. **Better performance** - Much more efficient than making separate API calls for each line

The new approach:
- Groups lines into batches (configurable size)
- Sends all prompts in a batch as a single API request
- Processes responses and maintains line order
- Falls back to individual calls if batch processing fails
- Much simpler code without threading complexity

You can adjust `BATCH_SIZE` based on your server's capabilities - larger batches will be more efficient but may hit token limits or timeout issues.

---

_**User**_

getting this error "An unexpected error occurred: Completions.create() got an unexpected keyword argument 'top_k'". You sure it is the same ":null,"code":400}(vllm-env) lukas@ubuntu:~$ curl http://localhost:8000/v1/chat/completions -H "Content-Type: application/json" -d '{
  "model": "/home/lukas/projects/LLM_testing/webui/text-generation-webui-main/models/Qwen3-0.6B-FP8",
  "messages": [
    {"role": "user", "content": "Give me a short introduction to large language models."}
  ],
  "temperature": 0.7,
  "top_p": 0.8,
  "top_k": 20,
  "max_tokens": 2000,
  "presence_penalty": 1.5,
  "chat_template_kwargs": {"enable_thinking": false}
}'" perhaps use requests and send the raw json, fix both the call qwen3 and the batch call qwen3 function please

---

_**Assistant**_

I'll fix both functions to use direct requests with raw JSON instead of the OpenAI client, which should resolve the issue with unsupported parameters.

```python:backend/utils/local_call.py
def call_qwen3(prompt: str, 
              temperature: float = 0,
              top_p: float = 0.8, 
              top_k: int = 20,
              presence_penalty: float = 1.5,
              enable_thinking: bool = False):
    """
    Makes a call to the local vLLM OpenAI-compatible server with Qwen3-specific parameters using direct HTTP requests.

    Args:
        prompt: The user prompt to send to the model.
        temperature: Controls randomness in generation (0 = deterministic).
        top_p: Nucleus sampling parameter.
        top_k: Top-k sampling parameter.
        presence_penalty: Penalty for tokens that have appeared in the text so far.
        enable_thinking: Whether to enable thinking mode in chat template.

    Returns:
        The content of the model's response, or None if an error occurs.
    """
    # --- Get Model Name Dynamically ---
    model_name = get_first_model_name(MODELS_ENDPOINT)
    if not model_name:
        print("Exiting as model name could not be determined.")
        return None
    # --- ---

    # --- Prepare Messages and Calculate Input Tokens using API ---
    system_message = {"role": "system", "content": "Du bist ein hilfreicher Assistent und Experte im Korrigieren von Text."}
    user_message = {"role": "user", "content": prompt}
    messages = [system_message, user_message]

    # Combine message content for tokenization
    text_for_tokenization = "\n".join([msg["content"] for msg in messages])

    input_tokens = get_token_count_from_api(model_name, text_for_tokenization)

    if input_tokens is not None:
        print(f"--- Input Token Count (from API): {input_tokens} ---")
    else:
        print("Warning: Failed to get token count from API. Cannot calculate max_tokens accurately.")
        input_tokens = -1

    # --- Calculate max_tokens ---
    if input_tokens != -1 and input_tokens < MODEL_CONTEXT_LIMIT:
        max_response_tokens = MODEL_CONTEXT_LIMIT - input_tokens - TOKEN_BUFFER
        if max_response_tokens <= 0:
            print(f"Error: Input prompt ({input_tokens} tokens) is too long for the model's context limit ({MODEL_CONTEXT_LIMIT} tokens). Cannot generate response.")
            return None
        print(f"--- Max Response Tokens Calculated: {max_response_tokens} ---")
    else:
        print("Warning: Using fallback max_tokens value (2000).")
        max_response_tokens = 2000
        if input_tokens >= MODEL_CONTEXT_LIMIT:
             print(f"Error: Input prompt ({input_tokens} tokens) likely exceeds model context limit ({MODEL_CONTEXT_LIMIT}). API call will probably fail.")
    # --- ---

    try:
        print(f"--- Sending prompt to {model_name} at {VLLM_API_BASE} ---")

        # Create request payload
        payload = {
            "model": model_name,
            "messages": messages,
            "max_tokens": max_response_tokens,
            "temperature": temperature,
            "top_p": top_p,
            "top_k": top_k,
            "presence_penalty": presence_penalty,
            "chat_template_kwargs": {"enable_thinking": enable_thinking}
        }

        headers = {
            "Content-Type": "application/json"
        }

        # Make direct HTTP request
        response = requests.post(
            f"{VLLM_API_BASE}/chat/completions",
            headers=headers,
            json=payload,
            timeout=60
        )
        
        response.raise_for_status()  # Raise exception for HTTP errors
        result = response.json()

        # Extract the response content
        if "choices" in result and result["choices"]:
            message_content = result["choices"][0]["message"]["content"]
            print("\n--- Model Response ---")
            print(message_content)
            if "usage" in result:
                 output_tokens = result["usage"]["completion_tokens"]
                 print(f"Reported Output Tokens: {output_tokens}")
            print("--------------------")
            return message_content
        else:
            print("Error: No response choices received.")
            print(f"Response: {result}")
            return None

    except requests.exceptions.RequestException as e:
        print(f"HTTP Request Error: {e}")
        if hasattr(e, 'response') and e.response:
            print(f"Response Status: {e.response.status_code}")
            print(f"Response Text: {e.response.text}")
        return None
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return None

def call_qwen3_batch(prompts: List[str], 
                   temperature: float = 0,
                   top_p: float = 0.8, 
                   top_k: int = 20,
                   presence_penalty: float = 1.5,
                   enable_thinking: bool = False) -> List[str]:
    """
    Makes a batch call to the local vLLM OpenAI-compatible server with direct HTTP requests.
    
    Args:
        prompts: List of user prompts to send to the model.
        temperature: Controls randomness in generation (0 = deterministic).
        top_p: Nucleus sampling parameter.
        top_k: Top-k sampling parameter.
        presence_penalty: Penalty for tokens that have appeared in the text so far.
        enable_thinking: Whether to enable thinking mode in chat template.

    Returns:
        List of response contents, or empty list if an error occurs.
    """
    if not prompts:
        return []
        
    # --- Get Model Name Dynamically ---
    model_name = get_first_model_name(MODELS_ENDPOINT)
    if not model_name:
        print("Exiting as model name could not be determined.")
        return []
    # --- ---

    try:
        print(f"--- Sending batch of {len(prompts)} prompts to {model_name} at {VLLM_API_BASE} ---")

        # For vLLM API, we may need to create multiple requests if the server doesn't support batching
        # First, try to create multiple requests in a single array of messages
        
        all_requests = []
        for prompt in prompts:
            request = {
                "model": model_name,
                "messages": [
                    {"role": "system", "content": "Du bist ein hilfreicher Assistent und Experte im Korrigieren von Text."},
                    {"role": "user", "content": prompt}
                ],
                "max_tokens": 2000,
                "temperature": temperature,
                "top_p": top_p,
                "top_k": top_k,
                "presence_penalty": presence_penalty,
                "chat_template_kwargs": {"enable_thinking": enable_thinking}
            }
            all_requests.append(request)
        
        batch_data = {
            "requests": all_requests
        }
        
        headers = {
            "Content-Type": "application/json"
        }
        
        # Try the batch endpoint first if available
        try:
            response = requests.post(
                f"{VLLM_API_BASE.replace('/v1', '')}/v1/batch/completions",
                headers=headers,
                json=batch_data,
                timeout=300  # Longer timeout for batch processing
            )
            
            response.raise_for_status()
            result = response.json()
            
            # Extract responses from batch format
            responses = []
            if "responses" in result:
                for resp in result["responses"]:
                    if "choices" in resp and resp["choices"] and "message" in resp["choices"][0]:
                        responses.append(resp["choices"][0]["message"]["content"])
                    else:
                        responses.append("")
                        
                print(f"--- Received {len(responses)} batch responses ---")
                return responses
                
        except requests.exceptions.RequestException as e:
            print(f"Batch endpoint failed or not available: {e}")
            print("Trying sequential requests instead...")
        
        # If batch endpoint failed or not available, make sequential requests
        responses = []
        for i, request in enumerate(all_requests):
            try:
                print(f"Processing item {i+1}/{len(all_requests)}...")
                resp = requests.post(
                    f"{VLLM_API_BASE}/chat/completions",
                    headers=headers,
                    json=request,
                    timeout=60
                )
                
                resp.raise_for_status()
                result = resp.json()
                
                if "choices" in result and result["choices"]:
                    responses.append(result["choices"][0]["message"]["content"])
                else:
                    responses.append("")
            except Exception as e:
                print(f"Error in request {i+1}: {e}")
                responses.append("")
                
        print(f"--- Received {len(responses)} sequential responses ---")
        return responses

    except Exception as e:
        print(f"Batch processing error: {e}")
        print("Falling back to individual calls...")
        
        # Last resort - use the single call function for each prompt
        responses = []
        for i, prompt in enumerate(prompts):
            print(f"Individual call {i+1}/{len(prompts)}...")
            result = call_qwen3(
                prompt=prompt,
                temperature=temperature,
                top_p=top_p,
                top_k=top_k,
                presence_penalty=presence_penalty,
                enable_thinking=enable_thinking
            )
            responses.append(result or "")
            
        return responses
```

These changes:

1. Replace the OpenAI client with direct HTTP requests using `requests`
2. Use the correct JSON structure matching your curl example
3. Fix the `top_k` parameter issue by including it directly in the JSON payload
4. Handle batch processing by attempting multiple strategies:
   - First try a batch endpoint if available
   - Fall back to sequential individual requests if batch endpoint fails
   - Use the single call function as a last resort
5. Improve error handling with more detailed error messages

The batch processing function is more robust now and should work even if your server doesn't have a dedicated batch endpoint. It will adapt to your server's capabilities and fall back to simpler methods if needed.

---

_**User**_

do you use these functions for max_tokens and model name?  you set max_tokens here hardcoded ""

---

_**Assistant**_



---

You are absolutely right! My apologies. In the `call_qwen3_batch` function, I did hardcode `max_tokens` to 2000 for each request within the batch, and I should have used the dynamic token calculation logic similar to the single `call_qwen3` function.

The `get_first_model_name` function IS being used correctly at the beginning of `call_qwen3_batch` to determine the `model_name`. However, the `max_tokens` calculation using `get_token_count_from_api` was indeed missed for the batch items.

Let me correct the `call_qwen3_batch` function to dynamically calculate `max_tokens` for each prompt in the batch. The `call_qwen3` (single prompt) function already does this correctly.

Here's the corrected `call_qwen3_batch` function in `backend/utils/local_call.py`:

```python:backend/utils/local_call.py
// ... existing code ...
def call_qwen3(prompt: str, 
              temperature: float = 0,
              top_p: float = 0.8, 
              top_k: int = 20,
              presence_penalty: float = 1.5,
              enable_thinking: bool = False):
// ... existing code ...
def call_qwen3_batch(prompts: List[str], 
                   temperature: float = 0,
                   top_p: float = 0.8, 
                   top_k: int = 20,
                   presence_penalty: float = 1.5,
                   enable_thinking: bool = False) -> List[str]:
    """
    Makes a batch call to the local vLLM OpenAI-compatible server with direct HTTP requests,
    dynamically calculating max_tokens for each prompt.
    
    Args:
        prompts: List of user prompts to send to the model.
        temperature: Controls randomness in generation (0 = deterministic).
        top_p: Nucleus sampling parameter.
        top_k: Top-k sampling parameter.
        presence_penalty: Penalty for tokens that have appeared in the text so far.
        enable_thinking: Whether to enable thinking mode in chat template.

    Returns:
        List of response contents, or empty list if an error occurs.
    """
    if not prompts:
        return []
        
    # --- Get Model Name Dynamically (once for the batch) ---
    model_name = get_first_model_name(MODELS_ENDPOINT)
    if not model_name:
        print("Exiting as model name could not be determined for batch.")
        return []
    # --- ---

    try:
        print(f"--- Preparing batch of {len(prompts)} prompts for {model_name} at {VLLM_API_BASE} ---")
        
        all_requests = []
        for i, prompt_text in enumerate(prompts):
            # --- Prepare Messages for this specific prompt ---
            system_message_content = "Du bist ein hilfreicher Assistent und Experte im Korrigieren von Text."
            user_message_content = prompt_text
            messages = [
                {"role": "system", "content": system_message_content},
                {"role": "user", "content": user_message_content}
            ]

            # --- Calculate Input Tokens and Max Response Tokens for this prompt ---
            text_for_tokenization = f"{system_message_content}\n{user_message_content}" # Simple concatenation for token counting
            input_tokens = get_token_count_from_api(model_name, text_for_tokenization)
            
            current_max_response_tokens = 2000 # Default fallback

            if input_tokens is not None:
                print(f"--- Prompt {i+1} Input Token Count (from API): {input_tokens} ---")
                if input_tokens < MODEL_CONTEXT_LIMIT:
                    calculated_max_tokens = MODEL_CONTEXT_LIMIT - input_tokens - TOKEN_BUFFER
                    if calculated_max_tokens > 0:
                        current_max_response_tokens = calculated_max_tokens
                        print(f"--- Prompt {i+1} Max Response Tokens Calculated: {current_max_response_tokens} ---")
                    else:
                        print(f"Error for prompt {i+1}: Input ({input_tokens} tokens) too long for context limit ({MODEL_CONTEXT_LIMIT}). Using fallback max_tokens.")
                else:
                    print(f"Error for prompt {i+1}: Input ({input_tokens} tokens) exceeds context limit ({MODEL_CONTEXT_LIMIT}). Using fallback max_tokens.")
            else:
                print(f"Warning for prompt {i+1}: Failed to get token count. Using fallback max_tokens ({current_max_response_tokens}).")
            # --- ---

            request_payload = {
                "model": model_name,
                "messages": messages,
                "max_tokens": current_max_response_tokens,
                "temperature": temperature,
                "top_p": top_p,
                "top_k": top_k,
                "presence_penalty": presence_penalty,
                "chat_template_kwargs": {"enable_thinking": enable_thinking}
            }
            all_requests.append(request_payload)
        
        batch_data = {
            # Note: The structure for a "batch" request can vary.
            # Some servers might expect a list of request objects directly.
            # Others might have a specific batch endpoint expecting a "requests" key.
            # We'll try the /v1/batch/completions endpoint first with a "requests" key.
            "requests": all_requests
        }
        
        headers = {
            "Content-Type": "application/json"
        }
        
        # Try the dedicated batch endpoint first
        try:
            print(f"--- Sending batch to dedicated batch endpoint: {VLLM_API_BASE.replace('/v1', '')}/v1/batch/completions ---")
            response = requests.post(
                f"{VLLM_API_BASE.replace('/v1', '')}/v1/batch/completions", # Common pattern for batch
                headers=headers,
                json=batch_data, # Send the structure with "requests": [...]
                timeout=300 
            )
            
            response.raise_for_status()
            result = response.json()
            
            responses = []
            # Assuming the batch endpoint returns a list of responses
            if isinstance(result, list): # If result is a list of individual responses
                 for item_result in result:
                    if "choices" in item_result and item_result["choices"] and "message" in item_result["choices"][0]:
                        responses.append(item_result["choices"][0]["message"]["content"])
                    else:
                        responses.append("") # Or handle error for this item
            elif "responses" in result and isinstance(result["responses"], list): # If result is {"responses": [...]}
                for item_result in result["responses"]:
                    # The structure of item_result here depends on your batch server's response
                    # This is a common pattern where each item in "responses" is a full completion object
                    if "body" in item_result and "choices" in item_result["body"] and item_result["body"]["choices"]:
                         responses.append(item_result["body"]["choices"][0]["message"]["content"])
                    elif "choices" in item_result and item_result["choices"] and "message" in item_result["choices"][0]: # More direct
                         responses.append(item_result["choices"][0]["message"]["content"])
                    else:
                        responses.append("")
            else: # Fallback if structure is unexpected
                print(f"Warning: Unexpected batch response format from batch endpoint. Result: {result}")
                # If the main /chat/completions endpoint supports a list of requests, it would be simpler.
                # For now, we assume the batch endpoint provides a clear list.
                # If not, the sequential fallback will be used.
                raise ValueError("Unexpected batch response format")


            print(f"--- Received {len(responses)} responses from batch endpoint ---")
            # Ensure we have a response for each request
            if len(responses) == len(prompts):
                return responses
            else:
                print(f"Warning: Mismatch in number of batch responses. Expected {len(prompts)}, got {len(responses)}. Falling back.")
                # This will trigger the fallback below

        except (requests.exceptions.RequestException, ValueError) as e:
            print(f"Dedicated batch endpoint failed or returned unexpected format: {e}")
            print("Attempting sequential requests to /chat/completions endpoint...")
        
        # Fallback: If batch endpoint failed, make sequential requests to the standard chat completions endpoint
        responses = []
        for i, single_request_payload in enumerate(all_requests):
            try:
                print(f"Sequential request {i+1}/{len(all_requests)} to /chat/completions...")
                resp = requests.post(
                    f"{VLLM_API_BASE}/chat/completions",
                    headers=headers,
                    json=single_request_payload, # Send one request payload at a time
                    timeout=60 
                )
                
                resp.raise_for_status()
                result = resp.json()
                
                if "choices" in result and result["choices"]:
                    responses.append(result["choices"][0]["message"]["content"])
                else:
                    responses.append("")
                    print(f"Warning: No choices in response for sequential request {i+1}. Response: {result}")
            except Exception as e_seq:
                print(f"Error in sequential request {i+1}: {e_seq}")
                responses.append("") # Append empty string for failed requests to maintain list length
                
        print(f"--- Received {len(responses)} responses from sequential requests ---")
        return responses

    except Exception as e_outer:
        print(f"Outer batch processing error: {e_outer}")
        # As a final fallback, though less efficient, call the single prompt function
        print("Final fallback: Using single call_qwen3 for each prompt...")
        final_responses = []
        for i, prompt_text in enumerate(prompts):
            print(f"Final fallback call {i+1}/{len(prompts)}...")
            result = call_qwen3( # This function already handles its own token calculation
                prompt=prompt_text,
                temperature=temperature,
                top_p=top_p,
                top_k=top_k,
                presence_penalty=presence_penalty,
                enable_thinking=enable_thinking
            )
            final_responses.append(result or "")
        return final_responses
// ... existing code ...
```

Key changes in `call_qwen3_batch`:
1.  **Dynamic `max_tokens` Calculation**: For each prompt in the input `prompts` list, it now:
    *   Constructs the `text_for_tokenization` (system + user message).
    *   Calls `get_token_count_from_api` to get `input_tokens`.
    *   Calculates `current_max_response_tokens` based on `MODEL_CONTEXT_LIMIT`, `input_tokens`, and `TOKEN_BUFFER`, similar to how `call_qwen3` does it.
    *   Uses this `current_max_response_tokens` in the payload for that specific request.
2.  **Model Name**: `get_first_model_name` is still called once at the beginning and the `model_name` is used for all individual request payloads created within the batch.
3.  **Batch Endpoint Logic**: I've refined the logic for handling responses from a dedicated batch endpoint (`/v1/batch/completions`), trying to anticipate a couple of common response structures. If this still doesn't match your server's actual batch response, the sequential fallback will be used.
4.  **Sequential Fallback Refined**: The sequential fallback now iterates through `all_requests` (which now contain correctly calculated `max_tokens`) and sends them one by one to the standard `/chat/completions` endpoint.
5.  **Final Fallback**: The last resort fallback to calling `call_qwen3` for each prompt remains, and `call_qwen3` itself correctly handles dynamic token calculation.

This should now ensure that `max_tokens` is set dynamically for each item even when using the `call_qwen3_batch` function. Thank you for catching that oversight!

---

_**User**_

thanks. also there is no batch endpoint, sent it all to the same. Is it that difficult to make 160 requests as fast as possible to the endpoint?

---

_**Assistant**_



---

Okay, I understand. If there's no dedicated batch endpoint, and you want to send many individual requests (like 160) as quickly as possible to the standard `/v1/chat/completions` endpoint, then we should use concurrency with `ThreadPoolExecutor`.

The previous `call_qwen3_batch` function was trying too many different batching strategies. Let's simplify it to do the following:
1.  Fetch the model name once.
2.  For each prompt, calculate its specific `max_tokens`.
3.  Create a JSON payload for each prompt.
4.  Use a `ThreadPoolExecutor` to send all these individual requests concurrently to the `/v1/chat/completions` endpoint.

This is similar to what we had in `md_process.py` with workers, but now it will be within the `call_qwen3_batch` function itself, and it will directly use `requests.post`.

Here's the updated `backend/utils/local_call.py`:

```python:backend/utils/local_call.py
// ... existing code ...
def get_first_model_name(endpoint: str) -> str | None:
// ... existing code ...
def get_token_count_from_api(model_id: str, text_to_tokenize: str) -> int | None:
// ... existing code ...
def call_qwen3(prompt: str, 
              temperature: float = 0,
              top_p: float = 0.8, 
              top_k: int = 20,
              presence_penalty: float = 1.5,
              enable_thinking: bool = False):
// ... existing code ...
from concurrent.futures import ThreadPoolExecutor, as_completed

def _make_single_chat_request(payload: dict, endpoint_url: str, request_index: int) -> tuple[int, str | None]:
    """
    Helper function to make a single HTTP request for chat completion.
    Returns the original index and the response content or None.
    """
    try:
        headers = {"Content-Type": "application/json"}
        response = requests.post(
            endpoint_url,
            headers=headers,
            json=payload,
            timeout=120  # Timeout for individual request
        )
        response.raise_for_status()
        result = response.json()
        if "choices" in result and result["choices"]:
            content = result["choices"][0]["message"]["content"]
            print(f"✓ Request {request_index} successful.")
            return request_index, content
        else:
            print(f"✗ Request {request_index}: No choices in response. Result: {result}")
            return request_index, None
    except requests.exceptions.RequestException as e:
        print(f"✗ Request {request_index} HTTP Error: {e}")
        if hasattr(e, 'response') and e.response is not None:
            try:
                print(f"Response Status: {e.response.status_code}, Response Text: {e.response.text}")
            except Exception:
                print("Could not decode error response.")
        return request_index, None
    except Exception as e:
        print(f"✗ Request {request_index} Unexpected Error: {e}")
        return request_index, None

def call_qwen3_batch(prompts: List[str], 
                   temperature: float = 0,
                   top_p: float = 0.8, 
                   top_k: int = 20,
                   presence_penalty: float = 1.5,
                   enable_thinking: bool = False,
                   max_workers: int = 10) -> List[str | None]: # Return list of string or None
    """
    Makes multiple concurrent calls to the local vLLM OpenAI-compatible server 
    (/v1/chat/completions) using direct HTTP requests. Dynamically calculates 
    max_tokens for each prompt.

    Args:
        prompts: List of user prompts to send to the model.
        temperature: Controls randomness.
        top_p: Nucleus sampling.
        top_k: Top-k sampling.
        presence_penalty: Presence penalty.
        enable_thinking: Chat template thinking mode.
        max_workers: Maximum number of concurrent requests.

    Returns:
        List of response contents (str) or None for failures, in the original order.
    """
    if not prompts:
        return []
        
    model_name = get_first_model_name(MODELS_ENDPOINT)
    if not model_name:
        print("Exiting as model name could not be determined for batch.")
        # Return list of Nones matching prompts length
        return [None] * len(prompts)
    
    print(f"--- Preparing {len(prompts)} concurrent requests for {model_name} at {VLLM_API_BASE}/chat/completions ---")
    
    request_payloads_with_indices = []
    for i, prompt_text in enumerate(prompts):
        system_message_content = "Du bist ein hilfreicher Assistent und Experte im Korrigieren von Text."
        user_message_content = prompt_text
        messages = [
            {"role": "system", "content": system_message_content},
            {"role": "user", "content": user_message_content}
        ]

        text_for_tokenization = f"{system_message_content}\n{user_message_content}"
        input_tokens = get_token_count_from_api(model_name, text_for_tokenization)
        
        current_max_response_tokens = 2000 

        if input_tokens is not None:
            # print(f"--- Prompt {i+1} Input Token Count (from API): {input_tokens} ---") # Can be verbose
            if input_tokens < MODEL_CONTEXT_LIMIT:
                calculated_max_tokens = MODEL_CONTEXT_LIMIT - input_tokens - TOKEN_BUFFER
                if calculated_max_tokens > 0:
                    current_max_response_tokens = calculated_max_tokens
                else:
                    print(f"Warning for prompt {i+1}: Input ({input_tokens} tokens) too long for context limit ({MODEL_CONTEXT_LIMIT}). Using fallback max_tokens={current_max_response_tokens}.")
            else:
                 print(f"Warning for prompt {i+1}: Input ({input_tokens} tokens) exceeds context limit ({MODEL_CONTEXT_LIMIT}). Using fallback max_tokens={current_max_response_tokens}.")
        else:
            print(f"Warning for prompt {i+1}: Failed to get token count. Using fallback max_tokens={current_max_response_tokens}.")

        payload = {
            "model": model_name,
            "messages": messages,
            "max_tokens": current_max_response_tokens,
            "temperature": temperature,
            "top_p": top_p,
            "top_k": top_k,
            "presence_penalty": presence_penalty,
            "chat_template_kwargs": {"enable_thinking": enable_thinking}
        }
        request_payloads_with_indices.append({'payload': payload, 'index': i})

    results_map = {} # To store results with original index

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = []
        for item in request_payloads_with_indices:
            future = executor.submit(
                _make_single_chat_request, 
                item['payload'], 
                f"{VLLM_API_BASE}/chat/completions",
                item['index'] + 1 # For 1-based logging
            )
            futures.append(future)

        for future in as_completed(futures):
            try:
                index, content = future.result()
                results_map[index] = content
            except Exception as e:
                # This case should ideally be handled within _make_single_chat_request
                # but as a fallback:
                print(f"Error retrieving result from future: {e}")
                # We need to find which original index this future corresponds to if it errors out before _make_single_chat_request returns index
                # For now, this is less likely as _make_single_chat_request traps exceptions.
    
    # Reconstruct results in original order
    final_responses: List[str | None] = [None] * len(prompts)
    for i in range(len(prompts)):
        # The index from _make_single_chat_request is 0-based for the map key
        original_prompt_index = request_payloads_with_indices[i]['index']
        final_responses[original_prompt_index] = results_map.get(original_prompt_index)

    print(f"--- Completed {len(prompts)} concurrent requests ---")
    return final_responses


if __name__ == "__main__":
// ... existing code ...
```

Key changes in `call_qwen3_batch`:
1.  **No Dedicated Batch Endpoint**: Removed all logic attempting to call `/v1/batch/completions`.
2.  **Concurrency with `ThreadPoolExecutor`**:
    *   It now prepares individual request payloads for all prompts. Each payload includes dynamically calculated `max_tokens`.
    *   A new helper function `_make_single_chat_request` is introduced. This function takes a single payload, makes the `requests.post` call to the standard `/v1/chat/completions` endpoint, and handles the response or errors for that single request. It also takes the original `request_index` to help map results back.
    *   `ThreadPoolExecutor` is used to execute `_make_single_chat_request` for all payloads concurrently.
    *   The `max_workers` parameter controls the level of concurrency.
3.  **Result Ordering**: The results from concurrent calls are collected and re-ordered to match the original input prompt order. The function now returns `List[str | None]` to indicate that some requests might fail.
4.  **Error Handling**: `_make_single_chat_request` includes error handling for individual requests. The main batch function then assembles these potentially `None` results.
5.  **Return Type**: Changed to `List[str | None]` to accurately reflect that individual requests within the "batch" might fail.

The `md_process.py` script should then be adjusted to handle `None` values in the `batch_responses` list. If a response is `None`, it means that particular line failed, and the original line should be kept.

This approach directly addresses your requirement to send many individual requests as fast as possible to the single available endpoint by leveraging concurrency. You can adjust `max_workers` in the `call_qwen3_batch` call (or make it a parameter of `process_md_file_batch`) to tune performance based on your server and network capacity.

---

_**User**_

how can this happen for all messages? "Response Status: 400, Response Text: {"object":"error","message":"This model's maximum context length is 2048 tokens. However, you requested 2055 tokens (104 in the messages, 1951 in the completion). Please reduce the length of the messages or completion. None","type":"BadRequestError","param":null,"code":400}"

---

_**Assistant**_



---

