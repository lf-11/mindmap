<!-- Generated by SpecStory -->

# Configuring Local LLM for PDF Text Extraction (2025-05-26 12:48:06)

_**User**_

I have this file pages.py that uses marker to extract text from a pdf document. I have a locally hosted llm server that can be called with functions from the local_call.py script. I want you to write me a function for pages.py that cleans up the extracted text. The issue mainly is white space not being added. There should be two prompts, one in English, one in German instructing the llm to simply correct the parsed output of the text extractor and add white spaces where necessary. Temperature should be set to 0. Make use of batch capabilites of the server. Also there should be a tqdm bar when doing this in my terminal, so I see the progress. What should be processed? The output ZPO.md file, there one paragraph is a line.

---

_**Assistant**_

Looking at your code, you need to modify the `_run_marker_subprocess` function in `pages.py` to include the OpenAI service configuration when `use_llm` is True. Based on the marker documentation and your `local_call.py` file, here's how to adjust it:

```python:backend/processing/pages.py
def _run_marker_subprocess(pdf_file_path: str, output_dir: str, language: str = "de", use_llm: bool = False):
    cmd = [
        "marker_single",
        str(pdf_file_path),
        "--output_dir", str(output_dir),
        "--languages", language,
    ]

    # Add LLM configuration if use_llm is True
    if use_llm:
        cmd.extend([
            "--use_llm",
            "--llm_service", "marker.services.openai.OpenAIService",
            "--openai_base_url", "http://192.168.178.61:8000/v1",
            "--openai_api_key", "not-needed",
            # You might want to specify a model name if needed
            # "--openai_model", "your-model-name"
        ])
    
    print(f"Running Marker: {' '.join(cmd)}")
    process = subprocess.run(cmd, capture_output=True, text=True, encoding='utf-8')
    
    if process.returncode != 0:
        print(f"Marker command failed: {process.stderr}")
    return process.stdout, process.stderr, process.returncode
```

The key changes are:
1. Added `--use_llm` flag when `use_llm` parameter is True
2. Set `--llm_service` to `marker.services.openai.OpenAIService` to use the OpenAI-compatible service
3. Set `--openai_base_url` to your vLLM server URL (`http://192.168.178.61:8000/v1`)
4. Set `--openai_api_key` to `"not-needed"` as vLLM doesn't require authentication by default

You might also want to make the configuration more flexible by adding parameters to the function:

```python:backend/processing/pages.py
def _run_marker_subprocess(
    pdf_file_path: str, 
    output_dir: str, 
    language: str = "de", 
    use_llm: bool = False,
    llm_base_url: str = "http://192.168.178.61:8000/v1",
    llm_api_key: str = "not-needed"
):
    cmd = [
        "marker_single",
        str(pdf_file_path),
        "--output_dir", str(output_dir),
        "--languages", language,
    ]

    if use_llm:
        cmd.extend([
            "--use_llm",
            "--llm_service", "marker.services.openai.OpenAIService",
            "--openai_base_url", llm_base_url,
            "--openai_api_key", llm_api_key,
        ])
    
    print(f"Running Marker: {' '.join(cmd)}")
    process = subprocess.run(cmd, capture_output=True, text=True, encoding='utf-8')
    
    if process.returncode != 0:
        print(f"Marker command failed: {process.stderr}")
    return process.stdout, process.stderr, process.returncode
```

And update the `process_pdf_file` function signature to pass these parameters through:

```python:backend/processing/pages.py
def process_pdf_file(
    pdf_file_path_str: str, 
    processing_output_base_dir: str = DEFAULT_PROCESSING_OUTPUT_BASE_DIR, 
    language: str = "de",
    use_marker_llm: bool = False,
    cleanup_marker_output: bool = True,
    llm_base_url: str = "http://192.168.178.61:8000/v1",
    llm_api_key: str = "not-needed"
) -> tuple[str | None, list[str], str | None, str | None]:
    # ... existing code ...
    
    try:
        stdout, stderr, returncode = _run_marker_subprocess(
            str(pdf_file_path.resolve()), 
            str(final_content_parent_dir.resolve()),
            language,
            use_marker_llm,
            llm_base_url,
            llm_api_key
        )
        # ... rest of existing code ...
```

This way you can easily configure different LLM endpoints if needed while keeping the default as your local vLLM server.

---

_**User**_

thanks, but is this working? I set the use_llm to true but still get this as command executed Running Marker: marker_single /home/lukas/Shared/ZPO.pdf --output_dir /home/lukas/projects/pdf_mindmap/backend/processing/marker_output/ZPO_temp --languages de

---

_**Assistant**_

I see the issue. Looking at your code, there's a mismatch between parameter names:

1. In `_run_marker_subprocess()`, the parameter is named `use_llm`
2. But in `process_pdf_file()`, the parameter is named `use_marker_llm`

When you call `process_pdf_file()`, you're using `use_marker_llm=False` (the default), which is then passed to `_run_marker_subprocess()` as `use_llm`.

You need to fix this by either:

1. Changing the parameter name in `_run_marker_subprocess()` to match `use_marker_llm`, or
2. Setting `use_marker_llm=True` when calling `process_pdf_file()`

Here's how to fix it in the `__main__` section:

```python:backend/processing/pages.py
if __name__ == '__main__':
    # ...
    formatted_content, images, raw_md, err = process_pdf_file(
        test_pdf_file,
        cleanup_marker_output=False,
        use_marker_llm=True  # Set this to True to enable LLM
    )
```

Alternatively, you could make the default `True` in the `process_pdf_file()` function definition:

```python:backend/processing/pages.py
def process_pdf_file(
    pdf_file_path_str: str, 
    processing_output_base_dir: str = DEFAULT_PROCESSING_OUTPUT_BASE_DIR, 
    language: str = "de",
    use_marker_llm: bool = True,  # Change default to True
    cleanup_marker_output: bool = True,
    llm_base_url: str = "http://192.168.178.61:8000/v1",
    llm_api_key: str = "not-needed"
) -> tuple[str | None, list[str], str | None, str | None]:
```

The parameter name mismatch is causing your LLM configuration not to be applied.

---

_**User**_

thanks, but my server received no request. The flag was now included though. we may need to set the model? also remove the api key stuff. For how to find out the model, see the local_call.py file I pasted earlier

---

_**Assistant**_

You're right - we need to specify the model name. Looking at your `local_call.py` file, it gets the model name dynamically from the server by calling the `/v1/models` endpoint.

Let's modify the `_run_marker_subprocess` function to include the model parameter and remove the API key if it's "not-needed":

```python:backend/processing/pages.py
def _run_marker_subprocess(
    pdf_file_path: str, 
    output_dir: str, 
    language: str = "de", 
    use_llm: bool = True,
    llm_base_url: str = "http://192.168.178.61:8000/v1",
    llm_api_key: str = "not-needed",
    llm_model: str = None
):
    cmd = [
        "marker_single",
        str(pdf_file_path),
        "--output_dir", str(output_dir),
        "--languages", language,
    ]

    if use_llm:
        cmd.extend([
            "--use_llm",
            "--llm_service", "marker.services.openai.OpenAIService",
            "--openai_base_url", llm_base_url,
        ])
        
        # Only add API key if it's not the default "not-needed" value
        if llm_api_key != "not-needed":
            cmd.extend(["--openai_api_key", llm_api_key])
            
        # Add model name if provided
        if llm_model:
            cmd.extend(["--openai_model", llm_model])
    
    print(f"Running Marker: {' '.join(cmd)}")
    process = subprocess.run(cmd, capture_output=True, text=True, encoding='utf-8')
    
    if process.returncode != 0:
        print(f"Marker command failed: {process.stderr}")
    return process.stdout, process.stderr, process.returncode
```

Then update the `process_pdf_file` function to pass the model parameter:

```python:backend/processing/pages.py
def process_pdf_file(
    pdf_file_path_str: str, 
    processing_output_base_dir: str = DEFAULT_PROCESSING_OUTPUT_BASE_DIR, 
    language: str = "de",
    use_marker_llm: bool = False,
    cleanup_marker_output: bool = True,
    llm_base_url: str = "http://192.168.178.61:8000/v1",
    llm_api_key: str = "not-needed",
    llm_model: str = None
) -> tuple[str | None, list[str], str | None, str | None]:
    # ... existing code ...
    
    try:
        stdout, stderr, returncode = _run_marker_subprocess(
            str(pdf_file_path.resolve()), 
            str(final_content_parent_dir.resolve()),
            language,
            use_marker_llm,
            llm_base_url,
            llm_api_key,
            llm_model
        )
        # ... rest of existing code ...
```

Finally, update the call in the `__main__` section to include a model name:

```python:backend/processing/pages.py
if __name__ == '__main__':
    # ...
    formatted_content, images, raw_md, err = process_pdf_file(
        test_pdf_file,
        cleanup_marker_output=False,
        use_marker_llm=True,
        llm_model="your-model-name"  # Replace with your actual model name
    )
```

For the model name, you can either:

1. Hardcode it if you know it (e.g., "llama3")
2. Add code to dynamically fetch it from the server as shown in `local_call.py`

If you want to dynamically fetch the model name, you could add a helper function:

```python:backend/processing/pages.py
import requests

def get_model_name(base_url: str) -> str:
    """Get the first available model name from the vLLM server"""
    models_endpoint = f"{base_url.replace('/v1', '')}/v1/models"
    try:
        response = requests.get(models_endpoint, timeout=10)
        response.raise_for_status()
        models_data = response.json()
        
        if models_data and "data" in models_data and len(models_data["data"]) > 0:
            first_model_id = models_data["data"][0].get("id")
            if first_model_id:
                print(f"Found model: {first_model_id}")
                return first_model_id
        return None
    except Exception as e:
        print(f"Error fetching model name: {e}")
        return None
```

Then use it in the main section:

```python:backend/processing/pages.py
if __name__ == '__main__':
    # ...
    llm_base_url = "http://192.168.178.61:8000/v1"
    model_name = get_model_name(llm_base_url)
    
    formatted_content, images, raw_md, err = process_pdf_file(
        test_pdf_file,
        cleanup_marker_output=False,
        use_marker_llm=True,
        llm_model=model_name
    )
```

This should help ensure the correct model is being used with your local vLLM server.

---

_**User**_

will this address work if I run this on the same machine as the llm server, or does it have to be localhost: xxx?

---

_**Assistant**_

If you're running this on the same machine as the LLM server, it's better to use `localhost` or `127.0.0.1` instead of the specific IP address `192.168.178.61`. Using `localhost` ensures the connection stays within the same machine and doesn't go through the network stack unnecessarily.

Change the URL in your code:

```python:backend/processing/pages.py
        llm_base_url = "http://localhost:8000/v1"
```

This will be more reliable when running on the same machine and will avoid potential network-related issues. The `localhost` address always refers to the current machine, while IP addresses like `192.168.178.61` might change if network configurations change.

Also, make sure the port (8000) matches the one your vLLM server

---

_**User**_

thanks. So I have set the urls, also in my llm server logs, I can see the models endpoint being queried. However no actual llm processing is being done. What is the issue? The command looks fine. I know it is using the full file path, but as the vllm

---

_**Assistant**_

Looking at the command that's being executed, I notice something unusual: the model name is a full file path:

```
--openai_model /home/lukas/projects/LLM_testing/webui/text-generation-webui-main/models/Mistral-Small-Instruct-2409-Q6_K_L.gguf
```

This might be causing issues with your vLLM server. In most OpenAI-compatible APIs, the model name should be a simple identifier, not a file path. Let's check what's happening and fix it:

---

<details>
            <summary>Searched codebase "how is model name determined in local_call.py" • **25** results</summary>
        
| File | Lines |
|------|-------|
| `backend/utils/local_call.py` | L26-64 |
| `backend/utils/local_call.py` | L120-162 |
| `backend/processing/pages.py` | L193-211 |
| `backend/utils/local_call.py` | L1-26 |
| `backend/utils/local_call.py` | L179-219 |
| `.specstory/history/2025-05-26_10-48-configuring-local-llm-for-pdf-text-extraction.md` | L235-296 |
| `backend/utils/local_call.py` | L162-179 |
| `backend/utils/local_call.py` | L219-238 |
| `backend/utils/local_call.py` | L238-247 |
| `backend/utils/local_call.py` | L64-89 |
| `.specstory/history/2025-05-26_10-48-configuring-local-llm-for-pdf-text-extraction.md` | L129-181 |
| `backend/utils/local_call.py` | L89-120 |
| `lib_docs/marker/help.txt` | L151-176 |
| `.specstory/history/2025-05-26_10-48-configuring-local-llm-for-pdf-text-extraction.md` | L296-335 |
| `lib_docs/marker/help.txt` | L465-493 |
| `backend/processing/pages.py` | L211-244 |
| `lib_docs/marker/help.txt` | L761-792 |
| `.specstory/history/2025-05-26_10-48-configuring-local-llm-for-pdf-text-extraction.md` | L181-235 |
| `backend/processing/pages.py` | L16-54 |
| `.specstory/history/2025-05-26_10-48-configuring-local-llm-for-pdf-text-extraction.md` | L1-23 |
| `backend/processing/preprocessing.py` | L1-1 |
| `.specstory/history/2025-05-25_11-16-postgresql-database-setup-for-text-processing-tool.md` | L1228-1253 |
| `lib_docs/marker/config_help.txt` | L374-423 |
| `lib_docs/marker/config_help.txt` | L319-342 |
| `.specstory/history/2025-05-26_06-59-enhancing-pdf-table-extraction-to-markdown.md` | L44-76 |

</details>

---

Looking at the issue more closely, I see the problem. You're getting the model name from the server, but it's returning the full file path instead of a simple model identifier:

```
Found model: /home/lukas/projects/LLM_testing/webui/text-generation-webui-main/models/Mistral-Small-Instruct-2409-Q6_K_L.gguf
```

For text-generation-webui (which is what you seem to be using based on the path), it sometimes returns full file paths as model IDs. This might cause issues when Marker tries to use this full path.

Let's modify your `get_model_name` function to extract just the model name without the path:

```python:backend/processing/pages.py
def get_model_name(base_url: str) -> str:
    """Get the first available model name from the vLLM server"""
    models_endpoint = f"{base_url.replace('/v1', '')}/v1/models"
    try:
        response = requests.get(models_endpoint, timeout=10)
        response.raise_for_status()
        models_data = response.json()
        
        if models_data and "data" in models_data and len(models_data["data"]) > 0:
            first_model_id = models_data["data"][0].get("id")
            if first_model_id:
                # Extract just the model name without the path
                model_name = Path(first_model_id).stem
                print(f"Found model: {first_model_id}")
                print(f"Using simplified model name: {model_name}")
                return model_name
        return None
    except Exception as e:
        print(f"Error fetching model name: {e}")
        return None
```

This will extract just the model name "Mistral-Small-Instruct-2409-Q6_K_L" from the full path, which should work better.

Another approach is to specify a shorter, simpler model name manually:

```python:backend/processing/pages.py
if __name__ == '__main__':
    # ...
    llm_base_url = "http://localhost:8000/v1"
    # Instead of using the dynamic model name, use a simple identifier
    model_name = "mistral"  # or any other short name that your server recognizes
    
    formatted_content, images, raw_md, err = process_pdf_file(
        test_pdf_file,
        cleanup_marker_output=False,
        use_marker_llm=True,
        llm_model=model_name
    )
```

Additionally, let's implement the LLM cleanup function you originally asked for. This will be a separate function that uses the local LLM to clean up the extracted text:

```python:backend/processing/pages.py
from tqdm import tqdm
import re

def clean_extracted_text_with_llm(text: str, llm_base_url: str = "http://localhost:8000/v1", batch_size: int = 3, language: str = "de") -> str:
    """
    Use the local LLM to clean up extracted text, focusing on whitespace issues.
    
    Args:
        text: The extracted text to clean.
        llm_base_url: URL for the local LLM server.
        batch_size: Number of paragraphs to process in a single batch.
        language: Language of the text ('de' or 'en')
        
    Returns:
        Cleaned text with proper whitespace.
    """
    from backend.utils.local_call import call_local_vllm
    
    # Split the text into paragraphs (each line is considered a paragraph)
    paragraphs = text.strip().split('\n')
    
    # Skip empty paragraphs
    paragraphs = [p for p in paragraphs if p.strip()]
    
    # Create batches of paragraphs
    batches = [paragraphs[i:i + batch_size] for i in range(0, len(paragraphs), batch_size)]
    
    # Prepare prompts for German and English
    prompts = {
        "de": """Bitte korrigiere den folgenden Textabschnitt, indem du hauptsächlich Leerzeichen korrigierst. 
Der Text stammt aus einer PDF-Extraktion und hat oft fehlende Leerzeichen zwischen Wörtern oder nach Satzzeichen.
Verändere den Inhalt NICHT, korrigiere NUR die Leerzeichen. Gib nur den korrigierten Text zurück, ohne zusätzliche Erklärungen.

Text:
{}""",
        "en": """Please correct the following text section by primarily fixing whitespace issues.
The text comes from PDF extraction and often has missing spaces between words or after punctuation marks.
DO NOT change the content, ONLY fix the whitespace issues. Return only the corrected text without any additional explanations.

Text:
{}"""
    }
    
    # Select the appropriate prompt based on language
    prompt_template = prompts.get(language, prompts["en"])
    
    # Process each batch
    cleaned_paragraphs = []
    
    # Use tqdm to show progress
    for batch in tqdm(batches, desc="Cleaning text with LLM"):
        # Join paragraphs in the batch with a special separator
        batch_text = "\n\n---\n\n".join(batch)
        
        # Create the prompt
        prompt = prompt_template.format(batch_text)
        
        # Call the LLM
        response = call_local_vllm(prompt)
        
        if response:
            # Split the response back into paragraphs
            if "\n\n---\n\n" in batch_text:
                # If we used separators, split by them
                try:
                    result_paragraphs = response.split("\n\n---\n\n")
                    # If the splitting didn't work as expected, fall back to original count
                    if len(result_paragraphs) != len(batch):
                        # Try different separator patterns
                        patterns =

---

