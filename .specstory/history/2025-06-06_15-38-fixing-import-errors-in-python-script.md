<!-- Generated by SpecStory -->

# Fixing import errors in Python script (2025-06-06 17:38:55)

_**User**_

I have a python file to extract headlines from a .md document and correct their hierarchy using an llm. Now for this line, I get this error. My structure is this: in my project root, I have backend/processing/headlines as a dir, where there is llm_headline.py and headlines.py. I am executing headlines.py from this dir, so project root + backend/processing/headlines. What is the issue? Or do you need more information?

---

_**Assistant**_

Looking at your error, the issue is with the import path. When you're executing `headlines.py` from the `backend/processing/headlines` directory, Python doesn't know about the `backend` module because you're already inside that directory structure.

Since you're running the script from `backend/processing/headlines/`, the import should be a relative import or just import the module directly from the same directory.

Here's the fix for line 672 in `headlines.py`:

```python:backend/processing/headlines/headlines.py
// ... existing code ...
from llm_headline import process_errors_concurrently
// ... existing code ...
```

Or if you prefer an explicit relative import:

```python:backend/processing/headlines/headlines.py
// ... existing code ...
from .llm_headline import process_errors_concurrently
// ... existing code ...
```

The issue occurs because:
1. You're executing the script from `backend/processing/headlines/`
2. Python treats this as your current working directory
3. The import `from backend.processing.headlines.llm_headline` tries to find a `backend` module, but there's no such module in your Python path when running from that location
4. Since `llm_headline.py` is in the same directory as `headlines.py`, you can import it directly

Change that import line and the error should be resolved.

---

_**User**_

thanks. I have now a file in backend/processing/headlines/ZPO_headlines.md. I also have a file here. I would now like you to write me another code piece, that, based on the headlines, retrieves for each headline the belonging text and stores it in a database table. The code piece should be in backend/processing/headlines/ I want several the following table here "Store paragraphs: doc_id, page_id, paragraph_id. Numerically ordered, e.g. after 5 comes 6)
- kind (for now text, footnotes or other)
- text
- header layer 1 / 2 / 3 (as many columns as there are layers, write down name of headers); else set to None.
- file_path (for other kinds of paragraphs (images etc.)". The code snippet I want you to write then should parse ZPO_processed.md. Each line there contains a paragraph. Assign paragraph ids based on the line number in the .md file. Then go through the ZPO_headlines.md file that is in the same dir as the file I want you to code. Here, headers are stored in this format. So from the number after line, you can devise the paragraph id, then assign the 1, 2 etc. after the layer to the header_layer column in the table. I also have this file for db setup already, perhaps add to it so I can just execute it and any tables not there get added.

---

_**Assistant**_

I'll help you create a code piece to parse the headlines and text, then store them in a database table. Let me first add the table definition to your database setup file, then create the processing script.

First, let's add the table to your database setup:

```python:backend/database/setup_database.py
# ... existing code ...
    """
    CREATE TABLE IF NOT EXISTS page_processing_log (
        log_id SERIAL PRIMARY KEY,
        doc_id INTEGER NOT NULL,
        page_sequence_number INTEGER NOT NULL,
        method_id INTEGER NOT NULL,
        retrial_id INTEGER NOT NULL DEFAULT 1,
        input_data TEXT,
        output_data TEXT,
        status VARCHAR(50) DEFAULT 'pending', -- e.g., 'pending', 'processing', 'success', 'failed'
        error_message TEXT,
        processed_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
        FOREIGN KEY (doc_id, page_sequence_number) REFERENCES pages (doc_id, page_sequence_number) ON DELETE CASCADE,
        FOREIGN KEY (method_id) REFERENCES methods (method_id) ON DELETE CASCADE,
        UNIQUE (doc_id, page_sequence_number, method_id, retrial_id)
    );
    """,
    """
    CREATE TABLE IF NOT EXISTS paragraphs (
        doc_id INTEGER NOT NULL,
        page_id INTEGER,
        paragraph_id INTEGER NOT NULL,
        kind VARCHAR(50) DEFAULT 'text', -- 'text', 'footnotes', 'other'
        text TEXT,
        header_layer_1 TEXT,
        header_layer_2 TEXT,
        header_layer_3 TEXT,
        header_layer_4 TEXT,
        header_layer_5 TEXT,
        file_path TEXT, -- for images etc.
        PRIMARY KEY (doc_id, paragraph_id),
        FOREIGN KEY (doc_id) REFERENCES docs (doc_id) ON DELETE CASCADE
    );
    """
    # ... existing code ...
```

Now, let's create the processing script:

```python:backend/processing/headlines/parse_to_database.py
import os
import re
import psycopg2
from psycopg2 import sql

# Database configuration - should match your setup_database.py
DB_CONFIG = {
    "dbname": "pdf_reader",
    "user": "postgres",
    "password": "postgres",  # Make sure this matches your actual password
    "host": "192.168.178.61",
    "port": "5432"
}

class HeadlineParser:
    def __init__(self, doc_id=1):
        self.doc_id = doc_id
        self.headers = {}  # Will store current headers for each layer
        
    def parse_headlines_file(self, headlines_file):
        """Parse the headlines file to extract header information"""
        headers_map = {}  # paragraph_id -> {layer: header_text}
        
        with open(headlines_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line.startswith('Line '):
                    # Extract line number and layer information
                    # Format: "Line 7: Layer 1: I. Arten von SÃ¤umnisentscheidungen"
                    match = re.match(r'Line (\d+): Layer (\d+): (.+)', line)
                    if match:
                        line_num = int(match.group(1))
                        layer = int(match.group(2))
                        header_text = match.group(3)
                        
                        # Clean up any annotations like [INHERITED], [LLM_MODIFIED], etc.
                        header_text = re.sub(r'\s*\[.*?\]', '', header_text).strip()
                        
                        if line_num not in headers_map:
                            headers_map[line_num] = {}
                        headers_map[line_num][layer] = header_text
        
        return headers_map
    
    def parse_processed_md(self, processed_file, headers_map):
        """Parse the processed markdown file and create paragraph records"""
        paragraphs = []
        current_headers = {}  # Track current headers for each layer
        
        with open(processed_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        for paragraph_id, line in enumerate(lines, 1):
            text = line.strip()
            
            # Skip empty lines but still assign paragraph_id
            if not text:
                continue
            
            # Check if this line is a header
            if paragraph_id in headers_map:
                # Update current headers based on the header(s) found at this line
                for layer, header_text in headers_map[paragraph_id].items():
                    current_headers[layer] = header_text
                    # Clear deeper layers when a higher-level header is found
                    layers_to_clear = [l for l in current_headers.keys() if l > layer]
                    for l in layers_to_clear:
                        del current_headers[l]
            
            # Determine paragraph kind
            kind = 'text'  # Default to text, you can add logic to detect footnotes/other
            
            # Create paragraph record
            paragraph = {
                'doc_id': self.doc_id,
                'page_id': None,  # You can add logic to determine page_id if needed
                'paragraph_id': paragraph_id,
                'kind': kind,
                'text': text,
                'header_layer_1': current_headers.get(1),
                'header_layer_2': current_headers.get(2),
                'header_layer_3': current_headers.get(3),
                'header_layer_4': current_headers.get(4),
                'header_layer_5': current_headers.get(5),
                'file_path': None  # For non-text content
            }
            
            paragraphs.append(paragraph)
        
        return paragraphs
    
    def insert_paragraphs_to_db(self, paragraphs):
        """Insert paragraphs into the database"""
        conn = None
        try:
            print(f"Connecting to database...")
            conn = psycopg2.connect(**DB_CONFIG)
            cur = conn.cursor()
            
            # Clear existing paragraphs for this document
            cur.execute("DELETE FROM paragraphs WHERE doc_id = %s", (self.doc_id,))
            print(f"Cleared existing paragraphs for doc_id {self.doc_id}")
            
            # Insert new paragraphs
            insert_query = """
                INSERT INTO paragraphs (
                    doc_id, page_id, paragraph_id, kind, text,
                    header_layer_1, header_layer_2, header_layer_3, 
                    header_layer_4, header_layer_5, file_path
                ) VALUES (
                    %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s
                )
            """
            
            for paragraph in paragraphs:
                cur.execute(insert_query, (
                    paragraph['doc_id'],
                    paragraph['page_id'],
                    paragraph['paragraph_id'],
                    paragraph['kind'],
                    paragraph['text'],
                    paragraph['header_layer_1'],
                    paragraph['header_layer_2'],
                    paragraph['header_layer_3'],
                    paragraph['header_layer_4'],
                    paragraph['header_layer_5'],
                    paragraph['file_path']
                ))
            
            conn.commit()
            print(f"Successfully inserted {len(paragraphs)} paragraphs into database")
            
        except (Exception, psycopg2.DatabaseError) as error:
            print(f"Error while inserting paragraphs: {error}")
            if conn:
                conn.rollback()
        finally:
            if conn:
                cur.close()
                conn.close()
                print("Database connection closed.")

def main():
    # File paths
    base_path = "/home/lukas/projects/pdf_mindmap/backend/processing"
    processed_file = os.path.join(base_path, "marker_output/ZPO_temp/ZPO_processed.md")
    headlines_file = os.path.join(base_path, "headlines/ZPO_headlines.md")
    
    # Check if files exist
    if not os.path.exists(processed_file):
        print(f"Error: Processed file not found at {processed_file}")
        return
    
    if not os.path.exists(headlines_file):
        print(f"Error: Headlines file not found at {headlines_file}")
        return
    
    # Initialize parser
    parser = HeadlineParser(doc_id=1)  # You can change doc_id as needed
    
    print(f"Parsing headlines from {headlines_file}...")
    headers_map = parser.parse_headlines_file(headlines_file)
    print(f"Found {len(headers_map)} header entries")
    
    print(f"Parsing processed markdown from {processed_file}...")
    paragraphs = parser.parse_processed_md(processed_file, headers_map)
    print(f"Created {len(paragraphs)} paragraph records")
    
    # Insert into database
    print("Inserting paragraphs into database...")
    parser.insert_paragraphs_to_db(paragraphs)
    
    print("Processing complete!")

if __name__ == "__main__":
    main()
```

The code does the following:

1. **Parses the headlines file** (`ZPO_headlines.md`) to extract which lines contain headers and at what layer
2. **Processes the markdown file** (`ZPO_processed.md`) line by line, assigning paragraph IDs based on line numbers
3. **Tracks current headers** at each layer as it processes the document
4. **Creates paragraph records** with the appropriate header assignments
5. **Inserts everything into the database** with proper header layer assignments

Key features:
- Handles multiple header layers (up to 5, easily extensible)
- Clears deeper header layers when a higher-level header is encountered
- Assigns paragraph IDs based on line numbers in the processed file
- Includes database connection and error handling
- Clears existing paragraphs for the document before inserting new ones

To use this:
1. First run your updated `setup_database.py` to create the paragraphs table
2. Then run `parse_to_database.py` to process and insert your document

The script assumes `doc_id = 1` for now, but you can modify this as needed for your document management system.

---

_**User**_

thanks. We need to remove the table we just created because there is an error. Please give me the instructions to do that via cli. Then for the paragraphs table, don't create five columns for headers. Instead, create one column, headler_layer, where for headers, we store their layer. Leave it blank for paragraphs that are not headers.

---

_**Assistant**_

Here are the CLI instructions to remove the table:

```bash
# Connect to PostgreSQL
psql -h 192.168.178.61 -U postgres -d pdf_reader

# Once connected, drop the table
DROP TABLE IF EXISTS paragraphs;

# Exit PostgreSQL
\q
```

Now let's update the database setup file with the corrected table structure:

```python:backend/database/setup_database.py
# ... existing code ...
    """
    CREATE TABLE IF NOT EXISTS paragraphs (
        doc_id INTEGER NOT NULL,
        page_id INTEGER,
        paragraph_id INTEGER NOT NULL,
        kind VARCHAR(50) DEFAULT 'text', -- 'text', 'footnotes', 'other'
        text TEXT,
        header_layer INTEGER, -- stores the layer number for headers, NULL for regular paragraphs
        file_path TEXT, -- for images etc.
        PRIMARY KEY (doc_id, paragraph_id),
        FOREIGN KEY (doc_id) REFERENCES docs (doc_id) ON DELETE CASCADE
    );
    """
# ... existing code ...
```

The key change is replacing the five separate `header_layer_1` through `header_layer_5` columns with a single `header_layer INTEGER` column that:
- Stores the layer number (1, 2, 3, etc.) for headers
- Remains NULL for regular paragraphs that are not headers

This is a much cleaner design and makes queries easier to write.

---

_**User**_

thanks, does the send_db.py script account for this change already? so that there is only one column now

---

_**Assistant**_

No, the `send_db.py` script still uses the old structure with multiple header columns. You need to update it to work with the new single `header_layer` column. Here are the changes needed:

```python:backend/processing/headlines/send_db.py
# ... existing code ...
    def parse_processed_md(self, processed_file, headers_map):
        """Parse the processed markdown file and create paragraph records"""
        paragraphs = []
        
        with open(processed_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        for paragraph_id, line in enumerate(lines, 1):
            text = line.strip()
            
            # Skip empty lines but still assign paragraph_id
            if not text:
                continue
            
            # Check if this line is a header
            header_layer = None
            if paragraph_id in headers_map:
                # Get the layer number for this header (assuming only one layer per line)
                header_layer = list(headers_map[paragraph_id].keys())[0]
            
            # Determine paragraph kind
            kind = 'text'  # Default to text, you can add logic to detect footnotes/other
            
            # Create paragraph record
            paragraph = {
                'doc_id': self.doc_id,
                'page_id': None,  # You can add logic to determine page_id if needed
                'paragraph_id': paragraph_id,
                'kind': kind,
                'text': text,
                'header_layer': header_layer,  # Single column now
                'file_path': None  # For non-text content
            }
            
            paragraphs.append(paragraph)
        
        return paragraphs
    
    def insert_paragraphs_to_db(self, paragraphs):
        """Insert paragraphs into the database"""
        conn = None
        try:
            print(f"Connecting to database...")
            conn = psycopg2.connect(**DB_CONFIG)
            cur = conn.cursor()
            
            # Clear existing paragraphs for this document
            cur.execute("DELETE FROM paragraphs WHERE doc_id = %s", (self.doc_id,))
            print(f"Cleared existing paragraphs for doc_id {self.doc_id}")
            
            # Insert new paragraphs
            insert_query = """
                INSERT INTO paragraphs (
                    doc_id, page_id, paragraph_id, kind, text,
                    header_layer, file_path
                ) VALUES (
                    %s, %s, %s, %s, %s, %s, %s
                )
            """
            
            for paragraph in paragraphs:
                cur.execute(insert_query, (
                    paragraph['doc_id'],
                    paragraph['page_id'],
                    paragraph['paragraph_id'],
                    paragraph['kind'],
                    paragraph['text'],
                    paragraph['header_layer'],
                    paragraph['file_path']
                ))
            
            conn.commit()
            print(f"Successfully inserted {len(paragraphs)} paragraphs into database")
            
        except (Exception, psycopg2.DatabaseError) as error:
            print(f"Error while inserting paragraphs: {error}")
            if conn:
                conn.rollback()
        finally:
            if conn:
                cur.close()
                conn.close()
                print("Database connection closed.")

# ... existing code ...
```

The key changes:
1. Removed the tracking of `current_headers` since we're no longer storing hierarchical header information
2. Changed the paragraph structure to use a single `header_layer` field that stores the layer number (or `None` for non-headers)
3. Updated the database insert query to use the new column structure
4. Simplified the logic since we only need to check if a line is a header and what layer it is

---

_**User**_

thanks. Now I want to chain this up with my frontend. I have this json dummy there. And my frontend is able to create a nice tree based diagram from it. So please write me a python script, also the headlines folder, create_json.py, that from the project root dir, in folder mindmap-tool/testing creates a json from my paragraphs table, with all the paragraphs that have doc_id 1. As doc title, set Test title, then use the headers as nodes of type header, make sure the parent child structure is followed. Then for any paragraphs that aren't headers, make them of type content, set the label to the text in both header and content node case. Plan out the logic for this file, before you code it. Also if start from the first paragraph that is a header for now. We will add logic to handle text before the first header, later.

---

_**Assistant**_



---

